---
title: "New kernel for Metropolis Hasting (bridging the gap with variational inference)"
output: html_document
header-includes:
- \usepackage{bbm}
- \usepackage{amsmath}
- \usepackage{amsmath,amssymb,amsthm,mathrsfs,amsfonts,dsfont}
---
$\newcommand{\expec}{\mathbb{E}}$
$\newcommand{\prob}{\mathbb{P}}$
$\newcommand{\indic}{\mathbb{1}}$
$\DeclareMathOperator*{\argmax}{arg\,max}$

In this document, we will introduce a new proposal for our Metropolis hastings algorithm.
The construction of this kernel is based off of approximation methods that consist in approximating the incomplete log likelihood. The following methods are applicable to continuous and discrete hierarchical models (the likelihood is whether discrete or continuous and the prior on the latent variable is always continuous).

# The Model
We study a classical missing data problem where:

* The observed data is a continuous random variable $Y = (Y_i, 1\leq i \leq N)$ that has observed values $(y_i, 1\leq i \leq N)$ in $\mathcal{Y}$
* The latent data is a continuous random variable $\psi = (\psi_i, 1\leq i \leq N)$ that takes on the values $(\psi_i, 1\leq i \leq N)$ in $\mathcal{Z}$ and consists in $N$ independent variables
* The components $Y_i$ are generated independently of each other and from their corresponding $\psi_i$
* $\log p(y,\theta)$ is the incomplete data log-likelihood
* $\log p(y,z,\theta)$ is the complete data log-likelihood and obtained by augmenting the observed data with the missing data
* We'll call $P_{Y_i,\psi_i,\theta}$ and $P_{\psi_i|Y_i,\theta}$ the probability distributions associated to the densities $p(y_i,\psi_i,\theta)$ and $p(\psi_i|y_i,\theta)$

Our objective is to create samples from the posterior distribiution $P_{\psi_i|Y_i,\theta}$ for all individuals i and at a fixed model parameter $\theta$.


In this document, we will consider N i.i.d. observations $y=(y_i, 1\leqslant i \leqslant N)$, unobserved individuals parameters $\psi=(\psi_i, 1\leqslant i \leqslant N)$ and a vector of parameters $\theta$.
The goal is to find the parameter $\theta$ that maximizes the likelihood $p(y;\theta)$:

$$p(y;\theta) = \int_{}^{} p(y,\psi;\theta) \, \mathrm{d}\psi$$

We are going to restrict ourselves to models that belong to the exponential family.
The complete data log likelihood can be expressed as:

$$\log p(y, \psi; \theta) = -\phi(\theta) + \langle S(y,\psi){,} \Phi(\theta) \rangle$$

With $\langle \cdot{,} \cdot \rangle$ being the scalar product and $\phi(\theta)$, $\Phi(\theta)$ and $S(y,\psi)$ are known functions.\\
We consider a joint model for the observations $y = (y_i, 1\leqslant i \leqslant N)$ and the individual parameters $\psi = (\psi_i, 1\leqslant i \leqslant N)$:
$$p(y, \psi; \theta) = p(y|\psi; \theta)p(\psi; \theta)$$
Where $p(y|\psi; \theta)$ is the conditional distribution of the observations of individual $i$ and $p(\psi; \theta)$ the distribution of the individual parameters.
Also:

\begin{split}
& y_i = f(t_i)+\sigma\epsilon_i \textrm{ with $\epsilon_i\sim \mathcal{N}(0,1)$}\\
&\psi_i = c_i*\psi_{pop}+\eta_i\textrm{ with $\eta_i\sim \mathcal{N}(0,\Omega)$}
\end{split}
\end{equation}
$f$ is a non linear function solution of a PK-PD Ordinary Differential Equation.\\
As a result we have the following hierarchical model:

\begin{split}
& y_i|\psi_i \sim \mathcal{N}(f(t),\sigma)\\
& \psi_i \sim \mathcal{N}(\psi_{pop}, \Omega)
\end{split}


# Introduction to approximation methods
## Laplace approximation
Laplace approximation consists in approximating an integral of the form:
$$I(m) := \int{e^{mg(x)}dx}$$
Where $m \in \mathbb{R}$ and g is three times differentiable.

Based on a second order taylor expansion of the function g around a point $x_0$ we get:
$$g(x) = \underbrace{g(x_0) + \nabla g(x_0)(x-x_0) +\frac{1}{2}(x-x0)\nabla^2g(x_0)(x-x0)}_{=\tilde{g}(x)} + R(x)$$

Which results in an approximation of the integral $I(m)$ (easier integral and consider a multivariate gaussian which integral sums to 1):
$$I(m)=e^{mg(x_0)}\sqrt(\frac{(2\pi)^p}{m|-\nabla^2g(x_0)|}) e^{-1/2m\nabla g(x_0)\nabla^2g(x_0)^{-1}\nabla g(x_0)}$$

## Application to our missing data problem
In our context we can easily write the inomplete likelihood, that we are trying to approximate, as for all parameters $\theta$:
$$ 
\begin{split}
p(y,\theta) &= \int{p(y,z,\theta)dz}\\
& = \int{e^{\log p(y,z,\theta)}dz}
\end{split}
$$

In this case we identify $g(z) = \log p(y,z,\theta)$. In the sequel we can lose dependence on the parameter $\theta$ since the model parameter stays constant throughout the MCMC. We'll get it back when dealing with the SAEM algorithm.
Also
$$ 
\begin{split}
g(z) &=\log p(y|z)+\log p(z) \\
& = \log l(z) + \log h(z)
\end{split}
$$
For the first two methods that we will introduce (Laplace and First order conditional estimation methods), we do a taylor expansion around the Maximum a Posteriori (MAP) (also known as Empirical bayes estimate (EBE)). As a result $\nabla g(z_0)=0$ with $z_0 = \arg \max \limits_{z}p(y,z)$. In the third example, called First Order method, the taylor exapnsion is done around the expectation of the random variable z: $z_0 = \expec{z}$. Thus $\nabla g(z_0)$ is not equal to $0$.
Also, we will apply previous results with $m=1$


###Laplacian Method

Using last derivation of the approximation of the integrale $I(m)$ we can explicit an approximation of our incomplete log likelihood:
$$
\begin{split}
-2\log p(y) & \approx -p\log2\pi - 2\log p(y,z_0) + \log |-\nabla^2 g(z_0)|\\
& \approx - 2\log p(y|z_0) - 2\log p(z_0)  -p\log2\pi + \log |-\nabla^2 g(z_0)|
\end{split}
$$
Since this last equ tion approximates (Bayes)
$$
\log p(y) = \log p(y|z_0) + \log p(z_0) - \log p(z_0|y)
$$
We can say that $-p\log2\pi + \log |-\nabla^2 g(z_0)|$ approximates $- \log p(z_0|y)$. Notice that $-p\log2\pi + \log |-\nabla^2 g(z_0)|$ is the value of a multivariate gaussian centered in $z_0$ and of covariance $|-\nabla^2 g(z_0)|$ taken in $z=z_0$.
As a result the laplacian method consist in, through an approximation of the incomplete log likelihood, approximating the posterior distribution $P_{Z|Y}$ as a multivariate gaussian centered in $z_0$ and of covariance $|-\nabla^2 g(z_0)|$. Where:
$$
\nabla^2 g(z_0) = \nabla^2 \log l(z_0) + \nabla^2 \log h(z_0)
$$

With $\nabla^2 \log l(z_0) =\nabla^2 \log p(y|z_0)=\frac{\nabla^2 l(z_0)}{l(z_0)} - \frac{\nabla l(z_0)\nabla  l(z_0)}{l^2(z_0)} $ and $\nabla^2 \log h(z_0)=\nabla^2 \log p(z_0)=\Omega^{-1}$ since the prior on the latent variable $z$ is a gaussian of covariance $\Omega$.


$$Z \sim \mathcal{N}(Z_{MAP}, (\frac{\nabla^2 l(z_0)}{l(z_0)} - \frac{\nabla l(z_0)\nabla  l(z_0)}{l^2(z_0)} +\Omega^{-1})^{-1})$$

###FOCE (First order conditional estimation)

In the case where the calculus of $\nabla^2 \log l(z_0)=\nabla^2 \log p(y|z_0)$ is hard, Wang (2007) introduced an approximation :
$$
\begin{split}
\nabla^2 \log l(z_0) & \approx \mathbb{E}^Z(\nabla^2 \log l(z_0))\\
& \approx - \frac{\nabla l(z_0)\nabla  l(z_0)}{l^2(z_0)} = - \frac{\nabla f(z_0)\nabla f(z_0)}{\sigma^2}
\end{split}
$$
As a result the laplacian method consist in, through an approximation of the incomplete log likelihood, approximating the posterior distribution $P_{Z|Y}$ as a multivariate gaussian centered in $z_0$ and of covariance $|-\nabla^2 g(z_0)| = (- \frac{\nabla f(z_0)\nabla f(z_0)}{\sigma^2}+\omega^{-1})^{-1}$
This is the first method I implemented.

$$Z \sim \mathcal{N}(Z_{MAP}, (- \frac{\nabla l(z_0)\nabla  l(z_0)}{l^2(z_0)} +\Omega^{-1})^{-1})$$

###FO (First order)

This method consists in doing the TD around the mean of the prior (fixed effect in the context of our models). As a result the gradient of $g(z) = \log p(y,z)$ is not null in $z=z_0$.

$$Z \sim \mathcal{N}(Z_{MEAN}, (- \frac{\nabla l(z_{MEAN})\nabla  l(z_{MEAN})}{l^2(z_{MEAN})} +\Omega^{-1})^{-1})$$


#MCMC proposals in the context of non linear mixed effects models
###Yield (37 individuals 224 observations)

Beforehand, the standard approach is to approximate the body as a simple compartment models. In this example we will focus on a one-compartment model for theophylline following oral dose D at time $t=0$ leading to description of concentration $y(t_i)$ at time $t_i \geq 0$ (i varies from 1 to N and denote the individual of the population):
$$
y_i = y(t_i) = f(\psi_i)+ \epsilon_i
$$
With :

$$f(\psi_i) = Y_{max} +B(x-X_{max})$$
Where $\psi = (X_{max},Y_{max},B)$


#SAEM coupled with an MCMC procedure for non linear mixed effects models


```{r}
mixt.simulate <-function(n,weight,mu,sigma)
{
  G <- length(mu)
  Z <- sample(1:G, n, prob=weight, replace=T)
  x<-NULL
  for (g in 1:G)
  {
    x<-c(x,rnorm(length(which(Z==g)),mu[g],sigma[g]))
  }
  return(x)
}

n <- 1000
weight<-c(0.7, 0.3) 
mu<-c(0,4)
sigma<-c(2,1)*1

x <- matrix(0,nrow=n,ncol=1)
x <- mixt.simulate(n,weight,mu,sigma)

plot(x)

```
