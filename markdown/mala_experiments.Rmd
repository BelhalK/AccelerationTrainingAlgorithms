---
title: "Mala experiments"
output: html_document
header-includes:
- \usepackage{bbm}
- \usepackage{amsmath}
- \usepackage{amsmath,amssymb,amsthm,mathrsfs,amsfonts,dsfont}
- \usepackage[utf8]{inputenc}
- \usepackage[T1]{fontenc}
- \usepackage[french]{babel}
- \usepackage{enumitem}
- \usepackage{amssymb}
---
$\newcommand{\expec}{\mathbb{E}}$
$\newcommand{\prob}{\mathbb{P}}$
$\newcommand{\indic}{\mathbb{1}}$
$\DeclareMathOperator*{\argmax}{arg\,max}$

In this document, we will deal with the MALA and variants algorithms such as: MALA, Nesterov, Adaptive MALA, Anisotropic MALA, Non reversible MALA

## The Model
First of all, let's define the model for a mixture of J distributions:
$$\text{N i.i.d. observations } y = (y_i, 1\leqslant i \leqslant N)$$
$$\text{J distributions } f_1, f_2, ..., f_J$$

The incomplete density for a single data point , $p(y_i)$ can be written as:
$$p(y_i) = \sum_{j=1}^{J}{\pi_jf_j(y_i)}$$
Now, we can consider parametrized distributions.
In that case, $f_j = f(.;\theta_j)$ where $\theta_j$ is the parameter associated to the distribution $f_j$.

The parameterized incomplete likelihood distribution is now equal to:
$$p(y_i; \theta) = \sum_{j=1}^{J}{\pi_jf(y_i; \theta_j)}$$

Where $\theta$ is the parameter of the incomplete likelihood and is equal to $(\pi_1,...,\pi_J,\theta_1,...,\theta_J)$.
In order to define the parameters $\pi_1,...,\pi_J$, known as the mixture weights, we need to introduce latent variables $\psi = (\psi_i, 1\leqslant i \leqslant N)$:
$$\psi_i \in \{1,...,J\}$$
$$\prob(\psi_i = j) = \pi_j$$ 
$$\text{with } \sum_{j=1}^{J}{\pi_j} = 1$$ 

We can now write the complete data likelihood for each individual $p(y_i, \psi_i; \theta)$:
$$p(y_i, \psi_i; \theta) = p(y_i| \psi_i; \theta)p(\psi_i; \theta)$$
$$\text{Since } p(y_i| \psi_i; \theta) = \prod_{j=1}^{J}{f(y_i,\theta_j)^{\indic_{\psi_i=j}}} \text{ and } p(\psi_i; \theta) = \prod_{j=1}^{J}{\pi_j^{\indic_{\psi_i=j}}}$$
$$p(y_i, \psi_i; \theta) = \prod_{j=1}^{J}{(\pi_j f(y_i,\theta_j))^{\indic_{\psi_i=j}}}$$

The complete log density for a single data point is thus:
$$\log p(y_i, \psi_i; \theta) = \sum_{j=1}^{J}{\indic_{\psi_i=j}(\log\pi_j + \log f(y_i,\theta_j))}$$

Let's simulate some data using R and plot them (for a mixture of 2 gaussians):

```{r}
mixt.simulate <-function(n,weight,mu,sigma)
{
  G <- length(mu)
  Z <- sample(1:G, n, prob=weight, replace=T)
  x<-NULL
  for (g in 1:G)
  {
    x<-c(x,rnorm(length(which(Z==g)),mu[g],sigma[g]))
  }
  return(x)
}

n <- 1000
weight<-c(0.7, 0.3) 
mu<-c(0,4)
sigma<-c(2,1)*1

x <- matrix(0,nrow=n,ncol=1)
x <- mixt.simulate(n,weight,mu,sigma)

plot(x)

```


dd



