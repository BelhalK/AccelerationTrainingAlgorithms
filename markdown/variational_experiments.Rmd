---
title: "EM and SAEM algorithm"
output: html_document
header-includes:
- \usepackage{bbm}
- \usepackage{amsmath}
- \usepackage{amsmath,amssymb,amsthm,mathrsfs,amsfonts,dsfont}
---
$\newcommand{\expec}{\mathbb{E}}$
$\newcommand{\prob}{\mathbb{P}}$
$\newcommand{\indic}{\mathbb{1}}$
$\DeclareMathOperator*{\argmax}{arg\,max}$

In this document, we will introduce the EM and SAEM algorithms.
First, in the general case and then applied to a mixture of gaussian distributions.
R code will be embedded throughout this notebook to see how each algorithm can be implemented.

## The Model
First of all, let's define the model for a mixture of J distributions:
$$\text{N i.i.d. observations } y = (y_i, 1\leqslant i \leqslant N)$$
$$\text{J distributions } f_1, f_2, ..., f_J$$

The incomplete density for a single data point , $p(y_i)$ can be written as:
$$p(y_i) = \sum_{j=1}^{J}{\pi_jf_j(y_i)}$$
Now, we can consider parametrized distributions.
In that case, $f_j = f(.;\theta_j)$ where $\theta_j$ is the parameter associated to the distribution $f_j$.

The parameterized incomplete likelihood distribution is now equal to:
$$p(y_i; \theta) = \sum_{j=1}^{J}{\pi_jf(y_i; \theta_j)}$$

Where $\theta$ is the parameter of the incomplete likelihood and is equal to $(\pi_1,...,\pi_J,\theta_1,...,\theta_J)$.
In order to define the parameters $\pi_1,...,\pi_J$, known as the mixture weights, we need to introduce latent variables $\psi = (\psi_i, 1\leqslant i \leqslant N)$:
$$\psi_i \in \{1,...,J\}$$
$$\prob(\psi_i = j) = \pi_j$$ 
$$\text{with } \sum_{j=1}^{J}{\pi_j} = 1$$ 

We can now write the complete data likelihood for each individual $p(y_i, \psi_i; \theta)$:
$$p(y_i, \psi_i; \theta) = p(y_i| \psi_i; \theta)p(\psi_i; \theta)$$
$$\text{Since } p(y_i| \psi_i; \theta) = \prod_{j=1}^{J}{f(y_i,\theta_j)^{\indic_{\psi_i=j}}} \text{ and } p(\psi_i; \theta) = \prod_{j=1}^{J}{\pi_j^{\indic_{\psi_i=j}}}$$
$$p(y_i, \psi_i; \theta) = \prod_{j=1}^{J}{(\pi_j f(y_i,\theta_j))^{\indic_{\psi_i=j}}}$$

The complete log density for a single data point is thus:
$$\log p(y_i, \psi_i; \theta) = \sum_{j=1}^{J}{\indic_{\psi_i=j}(\log\pi_j + \log f(y_i,\theta_j))}$$

Let's simulate some data using R and plot them (for a mixture of 2 gaussians):

```{r}
mixt.simulate <-function(n,weight,mu,sigma)
{
  G <- length(mu)
  Z <- sample(1:G, n, prob=weight, replace=T)
  x<-NULL
  for (g in 1:G)
  {
    x<-c(x,rnorm(length(which(Z==g)),mu[g],sigma[g]))
  }
  return(x)
}

n <- 1000
weight<-c(0.7, 0.3) 
mu<-c(0,4)
sigma<-c(2,1)*1

x <- matrix(0,nrow=n,ncol=1)
x <- mixt.simulate(n,weight,mu,sigma)

plot(x)

```



We can introduce by now another quantity of interest that will be used in the different algorithms: the conditional distribution of the latent variables given the observations.
$$p_{ij} = \prob(\psi_i = j|y_i;\theta)$$
Bayes rule gives:
$$p_{ij} =\frac{\prob(\psi_i = j)\prob(y_i|\psi_i;\theta)}{p(y_i; \theta)}=\frac{\pi_jf(y_i;\theta_j)}{\sum_{k=1}^{J}\pi_kf(y_i;\theta_k)}$$

We restrict this document to models that belongs to the exponential family. 
The complete data likelihood is given by:
$$p(y, \psi; \theta) = \exp\{-\phi(theta) + \langle S(y,\psi){,} \Phi(\theta) \rangle \}$$
With $\langle \cdot{,} \cdot \rangle$ being the scalar product and $\phi(\theta)$, $\Phi(\theta)$ and $S(y,\psi)$ are known functions.
$$p(y_i, \psi_i; \theta) = \prod_{j=1}^{J}{(\pi_j f(y_i,\theta_j))^{\indic_{\psi_i=j}}}$$

$$\begin{equation} \label{eq1}
\begin{split}
p(y_i, \psi_i; \theta) & = \prod_{j=1}^{J}{(\pi_j f(y_i,\theta_j))^{\indic_{\psi_i=j}}} \\
 & = \prod_{j=1}^{J}{e^{\indic_{\psi_i=j}\log(\pi_j)}e^{-\frac{\indic_{\psi_i=j}}{2}\log(2\pi\sigma_j^2)}e^{-\frac{\indic_{\psi_i=j}(y_i - \mu_j)^2}{2\sigma_j^2}}}
\end{split}
\end{equation}$$
Rewritting this product of exponentials as exponentials of a sum and summing over all the individuals $i$ we can write the complete model as:
$$p(y, \psi; \theta) = exp\{ \sum_{j=1}^{J}{\sum_{i=1}^{N}{(\indic_{\psi_i=j}(\log\pi_j - \frac{1}{2}\log(\pi_j\sigma_j^2) - \frac{\mu_j^2}{2\sigma_j^2}))}}
+ \sum_{j=1}^{J}{\sum_{i=1}^{N}{(\indic_{\psi_i=j}y_i\frac{\mu_j}{\sigma_j^2})}
-\sum_{j=1}^{J}{\sum_{i=1}^{N}{(\indic_{\psi_i=j}y_i^2 \frac{1}{2\sigma_j^2})}}}\}
$$


We recognize the scalar product of $\Phi(\theta)$ and $S(y,\psi)$ inside the exponential, where $S(y,\psi) = (S_1(y,\psi),S_2(y,\psi),S_3(y,\psi))$ with:
$$S_1(y,\psi) = \sum_{i=1}^{N}{\indic_{\psi_i=j}}$$
$$S_2(y,\psi) = \sum_{i=1}^{N}{\indic_{\psi_i=j}y_i}$$
$$S_3(y,\psi) = \sum_{i=1}^{N}{\indic_{\psi_i=j}y_i^2}$$

The complete log likelihood is:
$$\log(p(y, \psi; \theta)) = \langle S(y,\psi){,} \Phi(\theta) \rangle$$

We can notice that in the case where the latent variables are unkown, taking the conditional expectation of the complete log likelihood given the observations resumes in calculating the expectations of the component of the sufficient statistc $S(y,\psi)$ (by linearity of the scalar product and the expectation operator).  
Let's calculate those expectations and see how we can retrieve general formula for our parameters from this:
$$\expec(S_1(y,\psi)|y,\theta) = \expec(\sum_{i=1}^{N}{\indic_{\psi_i=j}}) = \sum_{i=1}^{N}{\expec(\indic_{\psi_i=j})} = \sum_{i=1}^{N}{\prob(\psi_i=j)} = N\pi_j $$
$$\expec(S_2(y,\psi)|y,\theta) = \expec(\sum_{i=1}^{N}{\indic_{\psi_i=j}y_i}) = \sum_{i=1}^{N}{\expec(\indic_{\psi_i=j}y_i)}$$
The law of total expectation gives:
$$\expec(S_2(y,\psi)|y,\theta) = \sum_{i=1}^{N}{P(\psi_i=j)E(\indic_{\psi_i=j}y_i|\psi_i=j)} = \sum_{i=1}^{N}{P(\psi_i=j)}\mu_j = \expec(\sum_{i=1}^{N}{\indic_{\psi_i=j}})\mu_j $$

And finally using the law of total expectation and the covariance definition:
$$\expec(S_3(y,\psi)|y,\theta) = \sum_{i=1}^{N}{P(\psi_i=j)E(\indic_{\psi_i=j}y_i^2|\psi_i=j)} = \expec(\sum_{i=1}^{N}{\indic_{\psi_i=j}})(\sigma_j^2+\mu_j^2)$$

Whether the latent variables are known (cf. SAEM) or not (cf. EM), these formulas stands. Of course in the former case, the expectation is the statistic itself and it the latter we need to compute its expectation.  
As a result, we can define a function $\Theta$ from the space of Borel functions and returning a value in $\mathbb{R}$.
$$
\begin{align*}
  \Theta \colon &\mathcal{H} \to \mathcal{R}\\      & S(.) \mapsto \theta.
\end{align*}
$$
With:
$$
\Theta(S(y,\psi)) = \Theta((S_1(y,\psi),S_2(y,\psi),S_3(y,\psi)) = \theta = (\pi_1,...,\pi_J,\mu_1,...,\mu_J,\sigma_1,...,\sigma_J)
)
$$
and:
$$\pi_j =\frac{1}{N} \expec(S_1(y,\psi)|y,\theta) $$
$$\mu_j = \frac{\expec(S_2(y,\psi)|y,\theta)}{\expec(S_1(y,\psi)|y,\theta)}$$
$$\sigma_j^2 = \frac{\expec(S_3(y,\psi)|y,\theta)}{\expec(S_1(y,\psi)|y,\theta)} - \mu_j^2$$



***
The following algorithms will be described in general and applied to the case where the mixture model is a mixture of gaussian distributions:  

$$\theta = (\pi_1,...,\pi_J,\mu_1,...,\mu_J,\sigma_1,...,\sigma_J) \text{ and } f(y_i,\theta_j) = \frac{1}{\sqrt{2\pi\sigma_j^2}}e^{-\frac{(y_i - \mu_j)^2}{2\sigma_j^2}}$$

***
## The algorithms

The goal here is to find the parameter $\theta$ that maximizes the model defined by the joint distribution $p(y;\theta)$.
We do what we call Maximum Likelihood Estimation (MLE).
$$\theta^{MLE} = \argmax p(y;\theta)$$
In the case of a mixture model, using this method can be impossible.  
The likelihood we are trying to maximize is:
$$p(y; \theta) = \prod_{i=1}^{N}{p(y_i; \theta)} \text{ since the ($y_i$) are i.i.d.}$$
the log likelihood is:
$$\log p(y; \theta) = \sum_{i=1}^{N}{\log(p(y_i; \theta))} = \sum_{i=1}^{N}{\log(\sum_{j=1}^{J}{\pi_jf(y_i;\theta_j)})}$$
Let's derive this expression with respect to $\theta_j$ for instance:
$$\frac{\partial \log(p(y; \theta))}{\partial \theta_j} = \sum_{i=1}^{N}{\frac{1}{\sum_{j=1}^{J}{\pi_jf(y_i;\theta_j)}}\pi_j\frac{df(y_i,\theta_j)}{d\theta_j}}$$
Solving $\frac{\partial \log(p(y; \theta))}{\partial \theta_j} = 0$ is pretty challenging.
Other methods can be used to find such an estimator: the EM or the SAEM algorithm.

### The EM algorithm

This algorithm simplifies the maximum likelihood estimation when it is difficult to directly compute it.  
General formulation  
In the general case, this algorithm consists in two step, starting with an initial value $\theta_0$ at each iteration k:

* step E: Evaluate the quantity $Q_k(\theta|\theta_{k-1})$ defined by:
$$Q_k(\theta|\theta_{k-1}) = \mathbb{E}(\log p(y,\psi;\theta)|y;\theta_{k-1})$$
* step M: Maximize the quantity $Q_k(\theta|\theta_{k-1})$ with respect to $\theta$:
$$\theta_k = \argmax Q_k(\theta|\theta_{k-1}) $$


Let's use the EM algorithm on our mixture model by evaluting first the conditional expectation of the complete log likelihood:
$$\mathbb{E}(\log p(y,\psi;\theta)|y;\theta_{k-1}) = \mathbb{E}(\sum_{i=1}^{N}{\sum_{j=1}^{J}{\indic_{\psi_i=j}(\log\pi_jf(y_i,\theta_j))|y;\theta_{k-1})}}$$
Using the linearity of the expectation operator:
$$\mathbb{E}(\log p(y,\psi;\theta)|y;\theta_{k-1}) = \sum_{i=1}^{N}{\sum_{j=1}^{J}{\mathbb{E}(\indic_{\psi_i=j}|y;\theta_{k-1})(\log\pi_jf(y_i,\theta_j)))}}$$

We notice that $\mathbb{E}(\indic_{\psi_i=j}|y;\theta_{k-1}) = \prob(\psi_i =j|y_i, \theta_{k-1}) = p_{ij}^{k-1}$  
As a result:
$$Q_k(\theta|\theta_{k-1}) = \sum_{i=1}^{N}{\sum_{j=1}^{J}{p_{ij}^{k-1}(\log\pi_jf(y_i,\theta_j)))}}$$
  
First, let's calculate the expecations of the three sufficient statistics:

$$\expec(S_1(y,\psi)|y,\theta) = \sum_{i=1}^{N}{\expec(\indic_{\psi_i=j}|y, \theta)} = \sum_{i=1}^{N}{p_{ij}}$$
$$\expec(S_2(y,\psi)|y,\theta) = \sum_{i=1}^{N}{\expec(\indic_{\psi_i=j}y_i|y, \theta)} = \sum_{i=1}^{N}{p_{ij}y_i}$$
$$\expec(S_3(y,\psi)|y,\theta) = \sum_{i=1}^{N}{\expec(\indic_{\psi_i=j}y_i^2|y, \theta)} = \sum_{i=1}^{N}{p_{ij}y_i^2}$$

Then we can use our function \Theta as we defined it earlier:
$$\Theta((S_{1}(y,\psi),S_{2}(y,\psi),S_{3}(y,\psi))) = (\pi_1,...,\pi_J,\mu_1,...,\mu_J,\sigma_1,...,\sigma_J) $$

And for every $l \in \{1,...,J\}$:
$$\pi_l = \frac{\sum_{i=1}^{N}{p_{il}}}{N}$$
$$\mu_l = \frac{\sum_{i=1}^{N}{p_{il}y_i}}{\sum_{i=1}^{N}{p_{il}}}$$
$$\sigma_l^2 = \frac{\sum_{i=1}^{N}{p_{il}y_i^2}}{\sum_{i=1}^{N}{p_{il}}}-\mu_l^2$$  



```{r, echo=FALSE}
library("ggplot2")
library("gridExtra")
library("reshape2")
source("mixtureAlgos.R")
source("mixtureFunctions.R")
theme_set(theme_bw())

n <- 100
weight<-c(0.7, 0.3) 
mu<-c(0,4)
sigma<-c(1,1)*1


weight0<-c(.5,.5)
mu0<-c(1,2)
sigma0<-c(.5,2)
K <- 300
K1 <- 20
alpha1 <- 0.7
alpha2 <- 0.4
seed0=44444


# ylim <- c(0.15, 0.5, 0.4)
ylim <- c(0.1, 0.3, 0.3)

M <- 1
nsim <- 5
#
G<-length(mu)
col.names <- c("iteration", paste0("p",1:G), paste0("mu",1:G), paste0("sigma",1:G))
theta<-list(p=weight,mu=mu,sigma=sigma)
# theta0<-list(p=weight0,mu=mu0,sigma=sigma0)
theta0<-theta


##  Simulation
x <- matrix(0,nrow=n,ncol=nsim)
for (j in (1:nsim))
{
  seed <- j*seed0
  set.seed(seed)
  xj<-mixt.simulate(n,weight,mu,sigma)
  x[,j] <- xj
}


## EM
dem <- NULL
df.em <- vector("list", length=nsim)
for (j in (1:nsim))
{ 
  df <- mixt.em(x[,j], theta, K)
  df <- mixt.ident(df)
  df$rep <- j
  dem <- rbind(dem,df)
  df$rep <- NULL
  df.em[[j]] <- df
}
graphConvMC(dem, title="EM")
```


### The SAEM algorithm

Since the latent data $\psi$ are not observed, we can not use $\log p(y, \psi; \theta)$.  
That's why we had to calculate the conditional expectation of this quantity in the EM algorithm.  
Here the principle of the SAEM algorithm is to simulate those latent variable and thus be able to use at each iteration of simulation k the distribution $\log p(y, \psi^k; \theta)$.  
As a result, the step E is now divided into two steps:  

* step S: Simulation of the latent variable $(\psi)^k$ from the conditional distribution $p(\psi|y; \theta_{k-1})$
* step SA: Stochastic approximation of the quantity of interest:  
$S_k(\theta|\theta_{k-1}) = S_{k-1}(\theta|\theta_{k-1}) + \gamma_k(\log p(y,\psi^k;\theta) - S_{k-1}(\theta|\theta_{k-1}))$ with $(\gamma_k)$ a decreasing sequence of positive numbers.


The stochastic approximation can be written again as:
$$s_{1,k}^j = S_{1,k-1}^j + \gamma_k(\sum_{i = 1}^{N}{\indic_{\psi_i^k=j}} - s_{1,k-1}^j)$$
$$s_{2,k}^j = s_{2,k-1}^j + \gamma_k(\sum_{i = 1}^{N}{y_i \indic_{\psi_i^k=j}} - s_{2,k-1}^j)$$
$$s_{3,k}^j = s_{3,k-1}^j + \gamma_k(\sum_{i = 1}^{N}{y_i^2 \indic_{\psi_i^k=j}} - s_{3,k-1}^j)$$

The M step remains the same:  
$$\Theta((s_{1,k}^j,s_{2,k}^j,s_{3,k}^j)) = (\pi_1,...,\pi_J,\mu_1,...,\mu_J,\sigma_1,...,\sigma_J) $$

And for every $l \in \{1,...,J\}$:
$$\pi_l = \frac{s_{1,k}^l}{N}$$
$$\mu_l = \frac{s_{2,k}^l}{s_{1,k}^l}$$
$$\sigma_l^2 = \frac{s_{3,k}^l}{s_{1,k}^l}-\mu_l^2$$  

```{r, echo=FALSE}
dem <- NULL
df.em <- vector("list", length=nsim)
for (j in (1:nsim))
{
  df <- mixt.saem2(x[,j], theta0, K, K1, M=1, alpha=0.6)
  df <- mixt.ident(df)
  df$rep <- j
  dem <- rbind(dem,df)
  df$rep <- NULL
  df.em[[j]] <- df
}
graphConvMC(dem, title="SAEM")
```

And the difference
```{r, echo=FALSE}
diff <- NULL
for (j in (1:nsim))
{
  seed <- j*seed0
  set.seed(seed)
  df <- mixt.saem2(x[,j], theta0, K, K1, M=1, alpha=0.6)
  df <- mixt.ident(df)
  df <- df - df.em[[j]]
  df$iteration <- 0:K
  df$rep <- j
  diff <- rbind(diff,df)
}
graphConvMC(diff, title="SAEM-EM")
```

### The SAGA SAEM algorithm

In this new algorithm only a batch of latent data are simulated. The index of these simulated latent variables are randomly sampled with replacement at each iteration k+1:

Simulation phase:
$$i(k) \in \{1,...,N\}$$
$$card(i(k)) = pN (p \in [0,1])$$
$$(\psi_{i \in i(k)})^{k} \sim p(\psi_i|y_i,\theta_{k-1})$$
$$(\psi_{i \notin i(k)})^{k} = (\psi_{i \notin i(k)})^{k-1}$$

p being the percentage of individuals simulated at each iteration. p remains constant throughout the algorithm.  

Stochastic Approximation:
$$s1_{k}^j(\theta|\theta_{k-1}) = s1_{k-1}^j(\theta|\theta_k) + \gamma_k(\sum_{i=1}^{N}{\indic_{\psi_i^{k-1}=j}} + \sum_{i \in i(k)}^{}{(\indic_{\psi_i^k=j}-\indic_{\psi_i^{k-1}=j})} - s1_{k-1}^j(\theta|\theta_{k-1}))$$
 $$s2_{k}^j(\theta|\theta_{k-1}) = s2_{k-1}^j(\theta|\theta_k) + \gamma_k(\sum_{i=1}^{N}{y_i\indic_{\psi_i^{k-1}=j}} + \sum_{i \in i(k)}^{}{(y_i\indic_{\psi_i^{k}=j}-y_i\indic_{\psi_i^{k-1}=j})} - s2_{k-1}^j(\theta|\theta_{k-1}))$$
 $$s3_{k}^j(\theta|\theta_{k-1}) = s3_{k-1}^j(\theta|\theta_{k-1}) + \gamma_k(\sum_{i=1}^{N}{y_i^2\indic_{\psi_i^{k-1}=j}} + \sum_{i \in i(k)}^{}{(y_i^2\indic_{\psi_i^{k}=j}-y_i^2\indic_{\psi_i^{k-1}=j})} - s3_{k-1}^j(\theta|\theta_{k-1}))$$






