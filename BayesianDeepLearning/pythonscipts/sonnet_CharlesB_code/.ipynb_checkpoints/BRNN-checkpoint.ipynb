{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "_izB6AqMjttk"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import os\n",
    "\n",
    "# Dependency imports\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sonnet as snt\n",
    "import ptb_reader\n",
    "import python.custom_getters.bayes_by_backprop as bbb\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nest = tf.contrib.framework.nest\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "# Data settings.\n",
    "tf.flags.DEFINE_string(\"data_path\", \"/tmp/ptb_data/data\", \"path to PTB data.\")\n",
    "\n",
    "# Deep LSTM settings.\n",
    "tf.flags.DEFINE_integer(\"embedding_size\", 650, \"embedding size.\")\n",
    "tf.flags.DEFINE_integer(\"hidden_size\", 650, \"network layer size\")\n",
    "tf.flags.DEFINE_integer(\"n_layers\", 2, \"number of layers\")\n",
    "\n",
    "# Training settings.\n",
    "tf.flags.DEFINE_integer(\"num_training_epochs\", 70, \"number of training epochs\")\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 20, \"SGD minibatch size\")\n",
    "tf.flags.DEFINE_integer(\"unroll_steps\", 35, \"Truncated BPTT unroll length.\")\n",
    "tf.flags.DEFINE_integer(\"high_lr_epochs\", 20, \"Number of epochs with lr_start.\")\n",
    "tf.flags.DEFINE_float(\"lr_start\", 1.0, \"SGD learning rate initializer\")\n",
    "tf.flags.DEFINE_float(\"lr_decay\", 0.9, \"Polynomical decay power.\")\n",
    "\n",
    "# BBB settings.\n",
    "tf.flags.DEFINE_float(\"prior_pi\", 0.25, \"Determines the prior mixture weights.\")\n",
    "tf.flags.DEFINE_float(\"prior_sigma1\", np.exp(-1.0), \"Prior component 1 stddev.\")\n",
    "tf.flags.DEFINE_float(\"prior_sigma2\", np.exp(-7.0), \"Prior component 2 stddev.\")\n",
    "\n",
    "# Logging settings.\n",
    "tf.flags.DEFINE_integer(\"print_every_batches\", 500, \"Sample every x batches.\")\n",
    "tf.flags.DEFINE_string(\"logbasedir\", \"/tmp/bayesian_rnn\", \"directory for logs\")\n",
    "tf.flags.DEFINE_string(\"logsubdir\", \"run1\", \"subdirectory for this experiment.\")\n",
    "tf.flags.DEFINE_string(\n",
    "    \"mode\", \"train_test\",\n",
    "    \"What mode to run in. Options: ['train_only', 'test_only', 'train_test']\")\n",
    "\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "\n",
    "_LOADED = {}\n",
    "DataOps = collections.namedtuple(\"DataOps\", \"sparse_obs sparse_target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def _run_session_with_no_hooks(sess, *args, **kwargs):\n",
    "  \"\"\"Only runs of the training op should contribute to speed measurement.\"\"\"\n",
    "  return sess._tf_sess().run(*args, **kwargs)  # pylint: disable=protected-access\n",
    "\n",
    "\n",
    "def _get_raw_data(subset):\n",
    "  raw_data = _LOADED.get(subset)\n",
    "  if raw_data is not None:\n",
    "    return raw_data, _LOADED[\"vocab\"]\n",
    "  else:\n",
    "    train_data, valid_data, test_data, vocab = ptb_reader.ptb_raw_data(\n",
    "        FLAGS.data_path)\n",
    "    _LOADED.update({\n",
    "        \"train\": np.array(train_data),\n",
    "        \"valid\": np.array(valid_data),\n",
    "        \"test\": np.array(test_data),\n",
    "        \"vocab\": vocab\n",
    "    })\n",
    "    return _LOADED[subset], vocab\n",
    "\n",
    "\n",
    "class PTB(object):\n",
    "  \"\"\"Wraps the PTB reader of the TensorFlow tutorial.\"\"\"\n",
    "\n",
    "  def __init__(self, subset, seq_len, batch_size, name=\"PTB\"):\n",
    "    self.raw_data, self.word2id = _get_raw_data(subset)\n",
    "    self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "    self.seq_len = seq_len\n",
    "    self.batch_size = batch_size\n",
    "    self.name = name\n",
    "\n",
    "  def to_string(self, idx_seq, join_token=\" \"):\n",
    "    return join_token.join([self.id2word[idx] for idx in idx_seq])\n",
    "\n",
    "  def to_string_tensor(self, time_major_idx_seq_batch):\n",
    "    def p_func(input_idx_seq):\n",
    "      return self.to_string(input_idx_seq)\n",
    "    return tf.py_func(p_func, [time_major_idx_seq_batch[:, 0]], tf.string)\n",
    "\n",
    "  def __call__(self):\n",
    "    x_bm, y_bm = ptb_reader.ptb_producer(\n",
    "        self.raw_data, self.batch_size, self.seq_len, name=self.name)\n",
    "    x_tm = tf.transpose(x_bm, [1, 0])\n",
    "    y_tm = tf.transpose(y_bm, [1, 0])\n",
    "    return DataOps(sparse_obs=x_tm, sparse_target=y_tm)\n",
    "\n",
    "  @property\n",
    "  def num_batches(self):\n",
    "    return np.prod(self.raw_data.shape) // (self.seq_len * self.batch_size)\n",
    "\n",
    "  @property\n",
    "  def vocab_size(self):\n",
    "    return len(self.word2id)\n",
    "\n",
    "\n",
    "class GlobalNormClippingOptimizer(tf.train.Optimizer):\n",
    "  \"\"\"Optimizer that clips gradients by global norm.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               opt,\n",
    "               clip_norm,\n",
    "               use_locking=False,\n",
    "               name=\"GlobalNormClippingOptimizer\"):\n",
    "    super(GlobalNormClippingOptimizer, self).__init__(use_locking, name)\n",
    "\n",
    "    self._opt = opt\n",
    "    self._clip_norm = clip_norm\n",
    "\n",
    "  def compute_gradients(self, *args, **kwargs):\n",
    "    return self._opt.compute_gradients(*args, **kwargs)\n",
    "\n",
    "  def apply_gradients(self, grads_and_vars, *args, **kwargs):\n",
    "    if self._clip_norm == np.inf:\n",
    "      return self._opt.apply_gradients(grads_and_vars, *args, **kwargs)\n",
    "    grads, vars_ = zip(*grads_and_vars)\n",
    "    clipped_grads, _ = tf.clip_by_global_norm(grads, self._clip_norm)\n",
    "    return self._opt.apply_gradients(zip(clipped_grads, vars_), *args, **kwargs)\n",
    "\n",
    "\n",
    "class CustomScaleMixture(object):\n",
    "\n",
    "  def __init__(self, pi, sigma1, sigma2):\n",
    "    self.mu, self.pi, self.sigma1, self.sigma2 = map(\n",
    "        np.float32, (0.0, pi, sigma1, sigma2))\n",
    "\n",
    "  def log_prob(self, x):\n",
    "    n1 = tf.contrib.distributions.Normal(self.mu, self.sigma1)\n",
    "    n2 = tf.contrib.distributions.Normal(self.mu, self.sigma2)\n",
    "    mix1 = tf.reduce_sum(n1.log_prob(x), -1) + tf.log(self.pi)\n",
    "    mix2 = tf.reduce_sum(n2.log_prob(x), -1) + tf.log(np.float32(1.0 - self.pi))\n",
    "    prior_mix = tf.stack([mix1, mix2])\n",
    "    lse_mix = tf.reduce_logsumexp(prior_mix, [0])\n",
    "    return tf.reduce_sum(lse_mix)\n",
    "\n",
    "\n",
    "def custom_scale_mixture_prior_builder(getter, name, *args, **kwargs):\n",
    "  \"\"\"A builder for the gaussian scale-mixture prior of Fortunato et al.\n",
    "\n",
    "  Please see https://arxiv.org/abs/1704.02798, section 7.1\n",
    "\n",
    "  Args:\n",
    "    getter: The `getter` passed to a `custom_getter`. Please see the\n",
    "      documentation for `tf.get_variable`.\n",
    "    name: The `name` argument passed to `tf.get_variable`.\n",
    "    *args: Positional arguments forwarded by `tf.get_variable`.\n",
    "    **kwargs: Keyword arguments forwarded by `tf.get_variable`.\n",
    "\n",
    "  Returns:\n",
    "    An instance of `tf.contrib.distributions.Distribution` representing the\n",
    "    prior distribution over the variable in question.\n",
    "  \"\"\"\n",
    "  # This specific prior formulation doesn't need any of the arguments forwarded\n",
    "  # from `get_variable`.\n",
    "  del getter\n",
    "  del name\n",
    "  del args\n",
    "  del kwargs\n",
    "  return CustomScaleMixture(\n",
    "      FLAGS.prior_pi, FLAGS.prior_sigma1, FLAGS.prior_sigma2)\n",
    "\n",
    "\n",
    "def lstm_posterior_builder(getter, name, *args, **kwargs):\n",
    "  \"\"\"A builder for a particular diagonal gaussian posterior.\n",
    "\n",
    "  Args:\n",
    "    getter: The `getter` passed to a `custom_getter`. Please see the\n",
    "      documentation for `tf.get_variable`.\n",
    "    name: The `name` argument passed to `tf.get_variable`.\n",
    "    *args: Positional arguments forwarded by `tf.get_variable`.\n",
    "    **kwargs: Keyword arguments forwarded by `tf.get_variable`.\n",
    "\n",
    "  Returns:\n",
    "    An instance of `tf.contrib.distributions.Distribution` representing the\n",
    "    posterior distribution over the variable in question.\n",
    "  \"\"\"\n",
    "  del args\n",
    "  parameter_shapes = tf.contrib.distributions.Normal.param_static_shapes(\n",
    "      kwargs[\"shape\"])\n",
    "\n",
    "  # The standard deviation of the scale mixture prior.\n",
    "  prior_stddev = np.sqrt(\n",
    "      FLAGS.prior_pi * np.square(FLAGS.prior_sigma1) +\n",
    "      (1 - FLAGS.prior_pi) * np.square(FLAGS.prior_sigma2))\n",
    "\n",
    "  loc_var = getter(\n",
    "      \"{}/posterior_loc\".format(name),\n",
    "      shape=parameter_shapes[\"loc\"],\n",
    "      initializer=kwargs.get(\"initializer\"),\n",
    "      dtype=tf.float32)\n",
    "  scale_var = getter(\n",
    "      \"{}/posterior_scale\".format(name),\n",
    "      initializer=tf.random_uniform(\n",
    "          minval=np.log(np.exp(prior_stddev / 4.0) - 1.0),\n",
    "          maxval=np.log(np.exp(prior_stddev / 2.0) - 1.0),\n",
    "          dtype=tf.float32,\n",
    "          shape=parameter_shapes[\"scale\"]))\n",
    "  return tf.contrib.distributions.Normal(\n",
    "      loc=loc_var,\n",
    "      scale=tf.nn.softplus(scale_var) + 1e-5,\n",
    "      name=\"{}/posterior_dist\".format(name))\n",
    "\n",
    "\n",
    "def non_lstm_posterior_builder(getter, name, *args, **kwargs):\n",
    "  \"\"\"A builder for a particular diagonal gaussian posterior.\n",
    "\n",
    "  Args:\n",
    "    getter: The `getter` passed to a `custom_getter`. Please see the\n",
    "      documentation for `tf.get_variable`.\n",
    "    name: The `name` argument passed to `tf.get_variable`.\n",
    "    *args: Positional arguments forwarded by `tf.get_variable`.\n",
    "    **kwargs: Keyword arguments forwarded by `tf.get_variable`.\n",
    "\n",
    "  Returns:\n",
    "    An instance of `tf.contrib.distributions.Distribution` representing the\n",
    "    posterior distribution over the variable in question.\n",
    "  \"\"\"\n",
    "  del args\n",
    "  parameter_shapes = tf.contrib.distributions.Normal.param_static_shapes(\n",
    "      kwargs[\"shape\"])\n",
    "\n",
    "  # The standard deviation of the scale mixture prior.\n",
    "  prior_stddev = np.sqrt(\n",
    "      FLAGS.prior_pi * np.square(FLAGS.prior_sigma1) +\n",
    "      (1 - FLAGS.prior_pi) * np.square(FLAGS.prior_sigma2))\n",
    "\n",
    "  loc_var = getter(\n",
    "      \"{}/posterior_loc\".format(name),\n",
    "      shape=parameter_shapes[\"loc\"],\n",
    "      initializer=kwargs.get(\"initializer\"),\n",
    "      dtype=tf.float32)\n",
    "  scale_var = getter(\n",
    "      \"{}/posterior_scale\".format(name),\n",
    "      initializer=tf.random_uniform(\n",
    "          minval=np.log(np.exp(prior_stddev / 2.0) - 1.0),\n",
    "          maxval=np.log(np.exp(prior_stddev / 1.0) - 1.0),\n",
    "          dtype=tf.float32,\n",
    "          shape=parameter_shapes[\"scale\"]))\n",
    "  return tf.contrib.distributions.Normal(\n",
    "      loc=loc_var,\n",
    "      scale=tf.nn.softplus(scale_var) + 1e-5,\n",
    "      name=\"{}/posterior_dist\".format(name))\n",
    "\n",
    "\n",
    "def build_modules(is_training, vocab_size):\n",
    "  \"\"\"Construct the modules used in the graph.\"\"\"\n",
    "\n",
    "  # Construct the custom getter which implements Bayes by Backprop.\n",
    "  if is_training:\n",
    "    estimator_mode = tf.constant(bbb.EstimatorModes.sample)\n",
    "  else:\n",
    "    estimator_mode = tf.constant(bbb.EstimatorModes.mean)\n",
    "  lstm_bbb_custom_getter = bbb.bayes_by_backprop_getter(\n",
    "      posterior_builder=lstm_posterior_builder,\n",
    "      prior_builder=custom_scale_mixture_prior_builder,\n",
    "      kl_builder=bbb.stochastic_kl_builder,\n",
    "      sampling_mode_tensor=estimator_mode)\n",
    "  non_lstm_bbb_custom_getter = bbb.bayes_by_backprop_getter(\n",
    "      posterior_builder=non_lstm_posterior_builder,\n",
    "      prior_builder=custom_scale_mixture_prior_builder,\n",
    "      kl_builder=bbb.stochastic_kl_builder,\n",
    "      sampling_mode_tensor=estimator_mode)\n",
    "\n",
    "  embed_layer = snt.Embed(\n",
    "      vocab_size=vocab_size,\n",
    "      embed_dim=FLAGS.embedding_size,\n",
    "      custom_getter=non_lstm_bbb_custom_getter,\n",
    "      name=\"input_embedding\")\n",
    "\n",
    "  cores = [snt.LSTM(FLAGS.hidden_size,\n",
    "                    custom_getter=lstm_bbb_custom_getter,\n",
    "                    forget_bias=0.0,\n",
    "                    name=\"lstm_layer_{}\".format(i))\n",
    "           for i in six.moves.range(FLAGS.n_layers)]\n",
    "  rnn_core = snt.DeepRNN(\n",
    "      cores,\n",
    "      skip_connections=False,\n",
    "      name=\"deep_lstm_core\")\n",
    "\n",
    "  # Do BBB on weights but not biases of output layer.\n",
    "  output_linear = snt.Linear(\n",
    "      vocab_size, custom_getter={\"w\": non_lstm_bbb_custom_getter})\n",
    "  return embed_layer, rnn_core, output_linear\n",
    "\n",
    "\n",
    "def build_logits(data_ops, embed_layer, rnn_core, output_linear, name_prefix):\n",
    "  \"\"\"This is the core model logic.\n",
    "\n",
    "  Unrolls a Bayesian RNN over the given sequence.\n",
    "\n",
    "  Args:\n",
    "    data_ops: A `sequence_data.SequenceDataOps` namedtuple.\n",
    "    embed_layer: A `snt.Embed` instance.\n",
    "    rnn_core: A `snt.RNNCore` instance.\n",
    "    output_linear: A `snt.Linear` instance.\n",
    "    name_prefix: A string to use to prefix local variable names.\n",
    "\n",
    "  Returns:\n",
    "    A 3D time-major tensor representing the model's logits for a sequence of\n",
    "    predictions. Shape `[time_steps, batch_size, vocab_size]`.\n",
    "  \"\"\"\n",
    "  # Embed the input index sequence.\n",
    "  embedded_input_seq = snt.BatchApply(\n",
    "      embed_layer, name=\"input_embed_seq\")(data_ops.sparse_obs)\n",
    "\n",
    "  # Construct variables for holding the RNN state.\n",
    "  initial_rnn_state = nest.map_structure(\n",
    "      lambda t: tf.get_local_variable(  # pylint: disable long lambda warning\n",
    "          \"{}/rnn_state/{}\".format(name_prefix, t.op.name), initializer=t),\n",
    "      rnn_core.initial_state(FLAGS.batch_size))\n",
    "  assign_zero_rnn_state = nest.map_structure(\n",
    "      lambda x: x.assign(tf.zeros_like(x)), initial_rnn_state)\n",
    "  assign_zero_rnn_state = tf.group(*nest.flatten(assign_zero_rnn_state))\n",
    "\n",
    "  # Unroll the RNN core over the sequence.\n",
    "  rnn_output_seq, rnn_final_state = tf.nn.dynamic_rnn(\n",
    "      cell=rnn_core,\n",
    "      inputs=embedded_input_seq,\n",
    "      initial_state=initial_rnn_state,\n",
    "      time_major=True)\n",
    "\n",
    "  # Persist the RNN state for the next unroll.\n",
    "  update_rnn_state = nest.map_structure(\n",
    "      tf.assign, initial_rnn_state, rnn_final_state)\n",
    "  with tf.control_dependencies(nest.flatten(update_rnn_state)):\n",
    "    rnn_output_seq = tf.identity(rnn_output_seq, name=\"rnn_output_seq\")\n",
    "  output_logits = snt.BatchApply(\n",
    "      output_linear, name=\"output_embed_seq\")(rnn_output_seq)\n",
    "  return output_logits, assign_zero_rnn_state\n",
    "\n",
    "\n",
    "def build_loss(model_logits, sparse_targets):\n",
    "  \"\"\"Compute the log loss given predictions and targets.\"\"\"\n",
    "  time_major_shape = [FLAGS.unroll_steps, FLAGS.batch_size]\n",
    "  flat_batch_shape = [FLAGS.unroll_steps * FLAGS.batch_size, -1]\n",
    "  xent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      logits=tf.reshape(model_logits, flat_batch_shape),\n",
    "      labels=tf.reshape(sparse_targets, flat_batch_shape[:-1]))\n",
    "  xent = tf.reshape(xent, time_major_shape)\n",
    "  # Sum over the sequence.\n",
    "  sequence_neg_log_prob = tf.reduce_sum(xent, axis=0)\n",
    "  # Average over the batch.\n",
    "  return tf.reduce_mean(sequence_neg_log_prob, axis=0)\n",
    "\n",
    "\n",
    "def train(logdir):\n",
    "  \"\"\"Run a network on the PTB training set, checkpointing the weights.\"\"\"\n",
    "\n",
    "  ptb_train = PTB(\n",
    "      name=\"ptb_train\",\n",
    "      subset=\"train\",\n",
    "      seq_len=FLAGS.unroll_steps,\n",
    "      batch_size=FLAGS.batch_size)\n",
    "\n",
    "  # Connect to training set.\n",
    "  data_ops = ptb_train()\n",
    "  embed_layer, rnn_core, output_linear = build_modules(\n",
    "      is_training=True, vocab_size=ptb_train.vocab_size)\n",
    "  prediction_logits, zero_state_op = build_logits(\n",
    "      data_ops, embed_layer, rnn_core, output_linear, name_prefix=\"train\")\n",
    "  data_loss = build_loss(prediction_logits, data_ops.sparse_target)\n",
    "\n",
    "  # Add the KL cost.\n",
    "  total_kl_cost = bbb.get_total_kl_cost()\n",
    "  num_dataset_elements = FLAGS.batch_size * ptb_train.num_batches\n",
    "  scaled_kl_cost = total_kl_cost / num_dataset_elements\n",
    "  total_loss = tf.add(scaled_kl_cost, data_loss)\n",
    "\n",
    "  # Optimize as usual.\n",
    "  global_step = tf.get_variable(\n",
    "      \"num_weight_updates\",\n",
    "      initializer=tf.constant(0, dtype=tf.int32, shape=()),\n",
    "      collections=[tf.GraphKeys.GLOBAL_VARIABLES, tf.GraphKeys.GLOBAL_STEP])\n",
    "\n",
    "  learning_rate = tf.get_variable(\n",
    "      \"lr\", initializer=tf.constant(FLAGS.lr_start, shape=(), dtype=tf.float32))\n",
    "  learning_rate_update = learning_rate.assign(learning_rate * FLAGS.lr_decay)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(\n",
    "      learning_rate=learning_rate)\n",
    "  optimizer = GlobalNormClippingOptimizer(optimizer, clip_norm=5.0)\n",
    "\n",
    "  with tf.control_dependencies([optimizer.minimize(total_loss)]):\n",
    "    global_step_and_train = global_step.assign_add(1)\n",
    "\n",
    "  # Connect to valid set.\n",
    "  ptb_valid = PTB(\n",
    "      name=\"ptb_valid\",\n",
    "      subset=\"valid\",\n",
    "      seq_len=FLAGS.unroll_steps,\n",
    "      batch_size=FLAGS.batch_size)\n",
    "  valid_data_ops = ptb_valid()\n",
    "  valid_logits, zero_valid_state = build_logits(\n",
    "      valid_data_ops, embed_layer, rnn_core, output_linear, name_prefix=\"valid\")\n",
    "  valid_loss = build_loss(valid_logits, valid_data_ops.sparse_target)\n",
    "\n",
    "  # Compute metrics for the sake of monitoring training.\n",
    "  predictions = tf.cast(\n",
    "      tf.argmax(prediction_logits, axis=-1), tf.int32, name=\"pred\")\n",
    "  correct_prediction_mask = tf.cast(\n",
    "      tf.equal(predictions, data_ops.sparse_target), tf.int32)\n",
    "  accuracy = tf.reduce_mean(\n",
    "      tf.cast(correct_prediction_mask, tf.float32), name=\"acc\")\n",
    "  error_rate = tf.subtract(1.0, accuracy, name=\"err\")\n",
    "  label_probs = tf.nn.softmax(prediction_logits, dim=-1)\n",
    "  predictive_entropy = tf.reduce_mean(\n",
    "      label_probs * tf.log(label_probs + 1e-12) * -1.0)\n",
    "\n",
    "  # Create tf.summary ops.\n",
    "  log_ops_to_run = {\n",
    "      \"scalar\": collections.OrderedDict([\n",
    "          (\"task_loss\", data_loss),\n",
    "          (\"train_err_rate\", error_rate),\n",
    "          (\"pred_entropy\", predictive_entropy),\n",
    "          (\"learning_rate\", learning_rate),\n",
    "          (\"elbo_loss\", total_loss),\n",
    "          (\"kl_cost\", total_kl_cost),\n",
    "          (\"scaled_kl_cost\", scaled_kl_cost),\n",
    "      ]),\n",
    "      \"text\": collections.OrderedDict([\n",
    "          (\"labels\", ptb_train.to_string_tensor(data_ops.sparse_target)),\n",
    "          (\"predictions\", ptb_train.to_string_tensor(predictions))\n",
    "      ])\n",
    "  }\n",
    "\n",
    "  for name, tensor in log_ops_to_run[\"scalar\"].items():\n",
    "    tf.summary.scalar(os.path.join(\"train\", name), tensor)\n",
    "\n",
    "  # The remaining logic runs the training loop and logging.\n",
    "  summary_writer = tf.summary.FileWriterCache.get(logdir=logdir)\n",
    "  tf.logging.info(\n",
    "      \"Beginning training for {} epochs, each with {} batches.\".format(\n",
    "          FLAGS.num_training_epochs, ptb_train.num_batches))\n",
    "  with tf.train.MonitoredTrainingSession(\n",
    "      is_chief=True, checkpoint_dir=logdir, save_summaries_secs=10) as sess:\n",
    "    num_updates_v = _run_session_with_no_hooks(sess, global_step)\n",
    "    epoch_idx_start, step_idx_start = divmod(\n",
    "        num_updates_v, ptb_train.num_batches)\n",
    "    tf.logging.info(\"On start, epoch: {}\\t step: {}\".format(\n",
    "        epoch_idx_start, step_idx_start))\n",
    "    for epoch_idx in six.moves.range(epoch_idx_start,\n",
    "                                     FLAGS.num_training_epochs):\n",
    "      tf.logging.info(\"Beginning Epoch {}/{}\".format(\n",
    "          epoch_idx, FLAGS.num_training_epochs))\n",
    "      tf.logging.info(\n",
    "          (\"Beginning by evaluating on the validation set, which has \"\n",
    "           \"{} batches.\".format(ptb_valid.num_batches)))\n",
    "      valid_cost = 0\n",
    "      valid_steps = 0\n",
    "      _run_session_with_no_hooks(sess, zero_valid_state)\n",
    "      for _ in six.moves.range(ptb_valid.num_batches):\n",
    "        valid_cost_v, num_updates_v = _run_session_with_no_hooks(\n",
    "            sess, [valid_loss, global_step])\n",
    "        valid_cost += valid_cost_v\n",
    "        valid_steps += FLAGS.unroll_steps\n",
    "      tf.logging.info(\"Validation set perplexity: {}\".format(\n",
    "          np.exp(valid_cost / valid_steps)))\n",
    "      summary = tf.summary.Summary()\n",
    "      summary.value.add(\n",
    "          tag=\"valid/word_level_perplexity\",\n",
    "          simple_value=np.exp(valid_cost / valid_steps))\n",
    "      summary_writer.add_summary(summary, num_updates_v)\n",
    "\n",
    "      # Run a training epoch.\n",
    "      epoch_cost = 0\n",
    "      epoch_steps = 0\n",
    "      for batch_idx in six.moves.range(step_idx_start, ptb_train.num_batches):\n",
    "        scalars_res, num_updates_v = sess.run(\n",
    "            [log_ops_to_run[\"scalar\"], global_step_and_train])\n",
    "        epoch_cost += scalars_res[\"task_loss\"]\n",
    "        epoch_steps += FLAGS.unroll_steps\n",
    "        if (batch_idx - 1) % FLAGS.print_every_batches == 0:\n",
    "          summary = tf.summary.Summary()\n",
    "          summary.value.add(\n",
    "              tag=\"train/word_level_perplexity\",\n",
    "              simple_value=np.exp(epoch_cost / epoch_steps))\n",
    "          summary_writer.add_summary(summary, num_updates_v)\n",
    "          scalars_res, strings_res = _run_session_with_no_hooks(\n",
    "              sess, [log_ops_to_run[\"scalar\"], log_ops_to_run[\"text\"]])\n",
    "          tf.logging.info(\"Num weight updates: {}\".format(num_updates_v))\n",
    "          for name, result in six.iteritems(scalars_res):\n",
    "            tf.logging.info(\"{}: {}\".format(name, result))\n",
    "          for name, result in six.iteritems(strings_res):\n",
    "            tf.logging.info(\"{}: {}\".format(name, result))\n",
    "\n",
    "      word_level_perplexity = np.exp(epoch_cost / epoch_steps)\n",
    "      tf.logging.info(\n",
    "          \"Train Perplexity after Epoch {}: {}\".format(\n",
    "              epoch_idx, word_level_perplexity))\n",
    "\n",
    "      end_of_epoch_fetches = [zero_state_op]\n",
    "      if epoch_idx >= FLAGS.high_lr_epochs:\n",
    "        end_of_epoch_fetches.append(learning_rate_update)\n",
    "      _run_session_with_no_hooks(sess, end_of_epoch_fetches)\n",
    "\n",
    "  tf.logging.info(\"Done training. Thanks for your time.\")\n",
    "\n",
    "\n",
    "def test(logdir):\n",
    "  \"\"\"Run a network on the PTB test set, restoring from the latest checkpoint.\"\"\"\n",
    "  global_step = tf.get_variable(\n",
    "      \"num_weight_updates\",\n",
    "      initializer=tf.constant(0, dtype=tf.int32, shape=()),\n",
    "      collections=[tf.GraphKeys.GLOBAL_VARIABLES, tf.GraphKeys.GLOBAL_STEP])\n",
    "\n",
    "  ptb_test = PTB(\n",
    "      name=\"ptb_test\",\n",
    "      subset=\"test\",\n",
    "      seq_len=FLAGS.unroll_steps,\n",
    "      batch_size=FLAGS.batch_size)\n",
    "\n",
    "  # Connect to test set.\n",
    "  data_ops = ptb_test()\n",
    "  # The variables in these modules will be restored from the checkpoint.\n",
    "  embed_layer, rnn_core, output_linear = build_modules(\n",
    "      is_training=False, vocab_size=ptb_test.vocab_size)\n",
    "  prediction_logits, _ = build_logits(\n",
    "      data_ops, embed_layer, rnn_core, output_linear, name_prefix=\"test\")\n",
    "  avg_nats_per_sequence = build_loss(prediction_logits, data_ops.sparse_target)\n",
    "\n",
    "  dataset_cost = 0\n",
    "  dataset_iters = 0\n",
    "  with tf.train.SingularMonitoredSession(checkpoint_dir=logdir) as sess:\n",
    "    tf.logging.info(\"Running on test set in {} batches.\".format(\n",
    "        ptb_test.num_batches))\n",
    "    tf.logging.info(\"The model has trained for {} steps.\".format(\n",
    "        _run_session_with_no_hooks(sess, global_step)))\n",
    "    for _ in range(ptb_test.num_batches):\n",
    "      dataset_cost += _run_session_with_no_hooks(sess, avg_nats_per_sequence)\n",
    "      dataset_iters += FLAGS.unroll_steps\n",
    "    tf.logging.info(\"Final test set perplexity: {}.\".format(\n",
    "        np.exp(dataset_cost / dataset_iters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Relational Memory Core Nth Farthest task",
   "provenance": [
    {
     "file_id": "1JB4nO80gE7mCfzRyZ0dzlQyT-3j4CkZP",
     "timestamp": 1527771383976
    },
    {
     "file_id": "1ZzjE4jgvpbOgU5Komjby0wDnVN6nsERA",
     "timestamp": 1523987033906
    },
    {
     "file_id": "1ySogp7DmX8r0Spru3e_yG2fThsVpxTlE",
     "timestamp": 1523633660599
    },
    {
     "file_id": "1HPXMYWSyUzyvGTXP780o7jlja84QZGi3",
     "timestamp": 1520546819048
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "deeptech",
   "language": "python",
   "name": "deeptech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
