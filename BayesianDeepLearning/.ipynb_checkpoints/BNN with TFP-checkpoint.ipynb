{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karimimohammedbelhal/Desktop/ongoing/hostnfly/deeptech/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tfp_bnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "  del argv  # unused\n",
    "  if tf.gfile.Exists(FLAGS.model_dir):\n",
    "    tf.logging.warning(\n",
    "        \"Warning: deleting old log directory at {}\".format(FLAGS.model_dir))\n",
    "    tf.gfile.DeleteRecursively(FLAGS.model_dir)\n",
    "  tf.gfile.MakeDirs(FLAGS.model_dir)\n",
    "\n",
    "  if FLAGS.fake_data:\n",
    "    mnist_data = build_fake_data()\n",
    "  else:\n",
    "    mnist_data = mnist.read_data_sets(FLAGS.data_dir, reshape=False)\n",
    "\n",
    "  (images, labels, handle,\n",
    "   training_iterator, heldout_iterator) = build_input_pipeline(\n",
    "       mnist_data, FLAGS.batch_size, mnist_data.validation.num_examples)\n",
    "\n",
    "  # Build a Bayesian LeNet5 network. We use the Flipout Monte Carlo estimator\n",
    "  # for the convolution and fully-connected layers: this enables lower\n",
    "  # variance stochastic gradients than naive reparameterization.\n",
    "  with tf.name_scope(\"bayesian_neural_net\", values=[images]):\n",
    "    neural_net = tf.keras.Sequential([\n",
    "        tfp.layers.Convolution2DFlipout(6,\n",
    "                                        kernel_size=5,\n",
    "                                        padding=\"SAME\",\n",
    "                                        activation=tf.nn.relu),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=[2, 2],\n",
    "                                     strides=[2, 2],\n",
    "                                     padding=\"SAME\"),\n",
    "        tfp.layers.Convolution2DFlipout(16,\n",
    "                                        kernel_size=5,\n",
    "                                        padding=\"SAME\",\n",
    "                                        activation=tf.nn.relu),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=[2, 2],\n",
    "                                     strides=[2, 2],\n",
    "                                     padding=\"SAME\"),\n",
    "        tfp.layers.Convolution2DFlipout(120,\n",
    "                                        kernel_size=5,\n",
    "                                        padding=\"SAME\",\n",
    "                                        activation=tf.nn.relu),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tfp.layers.DenseFlipout(84, activation=tf.nn.relu),\n",
    "        tfp.layers.DenseFlipout(10)\n",
    "        ])\n",
    "\n",
    "    logits = neural_net(images)\n",
    "    labels_distribution = tfd.Categorical(logits=logits)\n",
    "\n",
    "  # Compute the -ELBO as the loss, averaged over the batch size.\n",
    "  neg_log_likelihood = -tf.reduce_mean(labels_distribution.log_prob(labels))\n",
    "  kl = sum(neural_net.losses) / mnist_data.train.num_examples\n",
    "  elbo_loss = neg_log_likelihood + kl\n",
    "\n",
    "  # Build metrics for evaluation. Predictions are formed from a single forward\n",
    "  # pass of the probabilistic layers. They are cheap but noisy predictions.\n",
    "  predictions = tf.argmax(logits, axis=1)\n",
    "  accuracy, accuracy_update_op = tf.metrics.accuracy(\n",
    "      labels=labels, predictions=predictions)\n",
    "\n",
    "  # Extract weight posterior statistics for layers with weight distributions\n",
    "  # for later visualization.\n",
    "  names = []\n",
    "  qmeans = []\n",
    "  qstds = []\n",
    "  for i, layer in enumerate(neural_net.layers):\n",
    "    try:\n",
    "      q = layer.kernel_posterior\n",
    "    except AttributeError:\n",
    "      continue\n",
    "    names.append(\"Layer {}\".format(i))\n",
    "    qmeans.append(q.mean())\n",
    "    qstds.append(q.stddev())\n",
    "\n",
    "  with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "    train_op = optimizer.minimize(elbo_loss)\n",
    "\n",
    "  init_op = tf.group(tf.global_variables_initializer(),\n",
    "                     tf.local_variables_initializer())\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # Run the training loop.\n",
    "    train_handle = sess.run(training_iterator.string_handle())\n",
    "    heldout_handle = sess.run(heldout_iterator.string_handle())\n",
    "    for step in range(FLAGS.max_steps):\n",
    "      _ = sess.run([train_op, accuracy_update_op],\n",
    "                   feed_dict={handle: train_handle})\n",
    "\n",
    "      if step % 100 == 0:\n",
    "        loss_value, accuracy_value = sess.run(\n",
    "            [elbo_loss, accuracy], feed_dict={handle: train_handle})\n",
    "        print(\"Step: {:>3d} Loss: {:.3f} Accuracy: {:.3f}\".format(\n",
    "            step, loss_value, accuracy_value))\n",
    "\n",
    "      if (step+1) % FLAGS.viz_steps == 0:\n",
    "        # Compute log prob of heldout set by averaging draws from the model:\n",
    "        # p(heldout | train) = int_model p(heldout|model) p(model|train)\n",
    "        #                   ~= 1/n * sum_{i=1}^n p(heldout | model_i)\n",
    "        # where model_i is a draw from the posterior p(model|train).\n",
    "        probs = np.asarray([sess.run((labels_distribution.probs),\n",
    "                                     feed_dict={handle: heldout_handle})\n",
    "                            for _ in range(FLAGS.num_monte_carlo)])\n",
    "        mean_probs = np.mean(probs, axis=0)\n",
    "\n",
    "        image_vals, label_vals = sess.run((images, labels),\n",
    "                                          feed_dict={handle: heldout_handle})\n",
    "        heldout_lp = np.mean(np.log(mean_probs[np.arange(mean_probs.shape[0]),\n",
    "                                               label_vals.flatten()]))\n",
    "        print(\" ... Held-out nats: {:.3f}\".format(heldout_lp))\n",
    "\n",
    "        qm_vals, qs_vals = sess.run((qmeans, qstds))\n",
    "\n",
    "        if HAS_SEABORN:\n",
    "          plot_weight_posteriors(names, qm_vals, qs_vals,\n",
    "                                 fname=os.path.join(\n",
    "                                     FLAGS.model_dir,\n",
    "                                     \"step{:05d}_weights.png\".format(step)))\n",
    "\n",
    "          plot_heldout_prediction(image_vals, probs,\n",
    "                                  fname=os.path.join(\n",
    "                                      FLAGS.model_dir,\n",
    "                                      \"step{:05d}_pred.png\".format(step)),\n",
    "                                  title=\"mean heldout logprob {:.2f}\"\n",
    "                                  .format(heldout_lp))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeptech",
   "language": "python",
   "name": "deeptech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
