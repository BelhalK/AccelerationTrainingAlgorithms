{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karimimohammedbelhal/Desktop/ongoing/hostnfly/deeptech/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/karimimohammedbelhal/Desktop/ongoing/hostnfly/deeptech/lib/python2.7/site-packages/ipykernel_launcher.py',\n",
       " '-f',\n",
       " '/Users/karimimohammedbelhal/Library/Jupyter/runtime/kernel-d32737d1-e743-4cc6-94ab-5abbb54660e8.json']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Dependency imports\n",
    "from absl import flags\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "from matplotlib import figure  # pylint: disable=g-import-not-at-top\n",
    "from matplotlib.backends import backend_agg\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets import mnist\n",
    "\n",
    "# TODO(b/78137893): Integration tests currently fail with seaborn imports.\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "\n",
    "try:\n",
    "  import seaborn as sns  # pylint: disable=g-import-not-at-top\n",
    "  HAS_SEABORN = True\n",
    "except ImportError:\n",
    "  HAS_SEABORN = False\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "IMAGE_SHAPE = [28, 28, 1]\n",
    "\n",
    "flags.DEFINE_float(\"learning_rate\",\n",
    "                   default=0.001,\n",
    "                   help=\"Initial learning rate.\")\n",
    "flags.DEFINE_integer(\"max_steps\",\n",
    "                     default=300,\n",
    "                     help=\"Number of training steps to run.\")\n",
    "flags.DEFINE_integer(\"batch_size\",\n",
    "                     default=128,\n",
    "                     help=\"Batch size.\")\n",
    "flags.DEFINE_string(\"data_dir\",\n",
    "                    default=os.path.join(os.getenv(\"TEST_TMPDIR\", \"/tmp\"),\n",
    "                                         \"bayesian_neural_network/data\"),\n",
    "                    help=\"Directory where data is stored (if using real data).\")\n",
    "flags.DEFINE_string(\n",
    "    \"model_dir\",\n",
    "    default=os.path.join(os.getenv(\"TEST_TMPDIR\", \"/tmp\"),\n",
    "                         \"bayesian_neural_network/\"),\n",
    "    help=\"Directory to put the model's fit.\")\n",
    "flags.DEFINE_integer(\"viz_steps\",\n",
    "                     default=100,\n",
    "                     help=\"Frequency at which save visualizations.\")\n",
    "flags.DEFINE_integer(\"num_monte_carlo\",\n",
    "                     default=50,\n",
    "                     help=\"Network draws to compute predictive probabilities.\")\n",
    "flags.DEFINE_bool(\"fake_data\",\n",
    "                  default=None,\n",
    "                  help=\"If true, uses fake data. Defaults to real data.\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "import sys\n",
    "FLAGS(sys.argv, known_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-7fe175c1efd8>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/karimimohammedbelhal/Desktop/ongoing/hostnfly/deeptech/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/karimimohammedbelhal/Desktop/ongoing/hostnfly/deeptech/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/bayesian_neural_network/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/karimimohammedbelhal/Desktop/ongoing/hostnfly/deeptech/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/bayesian_neural_network/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/bayesian_neural_network/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/bayesian_neural_network/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/karimimohammedbelhal/Desktop/ongoing/hostnfly/deeptech/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "tf.gfile.MakeDirs(FLAGS.model_dir)\n",
    "mnist_data = mnist.read_data_sets(FLAGS.data_dir, reshape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_input_pipeline(mnist_data, batch_size, heldout_size):\n",
    "  \"\"\"Build an Iterator switching between train and heldout data.\"\"\"\n",
    "\n",
    "  # Build an iterator over training batches.\n",
    "  training_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "      (mnist_data.train.images, np.int32(mnist_data.train.labels)))\n",
    "  training_batches = training_dataset.shuffle(\n",
    "      50000, reshuffle_each_iteration=True).repeat().batch(batch_size)\n",
    "  training_iterator = training_batches.make_one_shot_iterator()\n",
    "\n",
    "  # Build a iterator over the heldout set with batch_size=heldout_size,\n",
    "  # i.e., return the entire heldout set as a constant.\n",
    "  heldout_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "      (mnist_data.validation.images,\n",
    "       np.int32(mnist_data.validation.labels)))\n",
    "  heldout_frozen = (heldout_dataset.take(heldout_size).\n",
    "                    repeat().batch(heldout_size))\n",
    "  heldout_iterator = heldout_frozen.make_one_shot_iterator()\n",
    "\n",
    "  # Combine these into a feedable iterator that can switch between training\n",
    "  # and validation inputs.\n",
    "  handle = tf.placeholder(tf.string, shape=[])\n",
    "  feedable_iterator = tf.data.Iterator.from_string_handle(\n",
    "      handle, training_batches.output_types, training_batches.output_shapes)\n",
    "  images, labels = feedable_iterator.get_next()\n",
    "\n",
    "  return images, labels, handle, training_iterator, heldout_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(images, labels, handle,\n",
    "   training_iterator, heldout_iterator) = build_input_pipeline(\n",
    "       mnist_data, FLAGS.batch_size, mnist_data.validation.num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"bayesian_neural_net\", values=[images]):\n",
    "    neural_net = tf.keras.Sequential([\n",
    "        tfp.layers.Convolution2DFlipout(6,\n",
    "                                        kernel_size=5,\n",
    "                                        padding=\"SAME\",\n",
    "                                        activation=tf.nn.relu),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=[2, 2],\n",
    "                                     strides=[2, 2],\n",
    "                                     padding=\"SAME\"),\n",
    "        tfp.layers.Convolution2DFlipout(16,\n",
    "                                        kernel_size=5,\n",
    "                                        padding=\"SAME\",\n",
    "                                        activation=tf.nn.relu),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=[2, 2],\n",
    "                                     strides=[2, 2],\n",
    "                                     padding=\"SAME\"),\n",
    "        tfp.layers.Convolution2DFlipout(120,\n",
    "                                        kernel_size=5,\n",
    "                                        padding=\"SAME\",\n",
    "                                        activation=tf.nn.relu),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tfp.layers.DenseFlipout(84, activation=tf.nn.relu),\n",
    "        tfp.layers.DenseFlipout(10)\n",
    "        ])\n",
    "\n",
    "    logits = neural_net(images)\n",
    "    labels_distribution = tfd.Categorical(logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neg_log_likelihood = -tf.reduce_mean(labels_distribution.log_prob(labels))\n",
    "kl = sum(neural_net.losses) / mnist_data.train.num_examples\n",
    "elbo_loss = neg_log_likelihood + kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = tf.argmax(logits, axis=1)\n",
    "accuracy, accuracy_update_op = tf.metrics.accuracy(labels=labels, predictions=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = []\n",
    "qmeans = []\n",
    "qstds = []\n",
    "for i, layer in enumerate(neural_net.layers):\n",
    "    try:\n",
    "        q = layer.kernel_posterior\n",
    "    except AttributeError:\n",
    "        continue\n",
    "names.append(\"Layer {}\".format(i))\n",
    "qmeans.append(q.mean())\n",
    "qstds.append(q.stddev())\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "    train_op = optimizer.minimize(elbo_loss)\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                     tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_lossadam = np.zeros(FLAGS.max_steps)\n",
    "train_accadam = np.zeros(FLAGS.max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(init_op)\n",
    "\n",
    "# Run the training loop.\n",
    "train_handle = sess.run(training_iterator.string_handle())\n",
    "heldout_handle = sess.run(heldout_iterator.string_handle())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for step in range(FLAGS.max_steps):\n",
    "for step in range(10):\n",
    "    _ = sess.run([train_op, accuracy_update_op],feed_dict={handle: train_handle})\n",
    "    train_lossadam[step],train_accadam[step] = sess.run([elbo_loss, accuracy],feed_dict={handle: train_handle})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = train_lossadam[0:100]\n",
    "#save loss\n",
    "with open('newlosses/adamloss', 'wb') as fp:\n",
    "    pickle.dump(adam, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BayesByBackprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = []\n",
    "qmeans = []\n",
    "qstds = []\n",
    "for i, layer in enumerate(neural_net.layers):\n",
    "    try:\n",
    "        q = layer.kernel_posterior\n",
    "    except AttributeError:\n",
    "        continue\n",
    "names.append(\"Layer {}\".format(i))\n",
    "qmeans.append(q.mean())\n",
    "qstds.append(q.stddev())\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "    train_op = optimizer.minimize(elbo_loss)\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                     tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_lossbbb = np.zeros(FLAGS.max_steps)\n",
    "train_accbbb = np.zeros(FLAGS.max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(init_op)\n",
    "\n",
    "# Run the training loop.\n",
    "train_handle = sess.run(training_iterator.string_handle())\n",
    "heldout_handle = sess.run(heldout_iterator.string_handle())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for step in range(FLAGS.max_steps):\n",
    "for step in range(10):\n",
    "    _ = sess.run([train_op, accuracy_update_op],feed_dict={handle: train_handle})\n",
    "    train_lossbbb[step],train_accbbb[step] = sess.run([elbo_loss, accuracy],feed_dict={handle: train_handle})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bbb = train_lossbbb[0:100]\n",
    "#save loss\n",
    "with open('newlosses/bbbloss', 'wb') as fp:\n",
    "    pickle.dump(bbb, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = []\n",
    "qmeans = []\n",
    "qstds = []\n",
    "for i, layer in enumerate(neural_net.layers):\n",
    "    try:\n",
    "        q = layer.kernel_posterior\n",
    "    except AttributeError:\n",
    "        continue\n",
    "names.append(\"Layer {}\".format(i))\n",
    "qmeans.append(q.mean())\n",
    "qstds.append(q.stddev())\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "    train_op = optimizer.minimize(elbo_loss)\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                     tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_lossmisso = np.zeros(FLAGS.max_steps)\n",
    "train_accmisso = np.zeros(FLAGS.max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(init_op)\n",
    "\n",
    "# Run the training loop.\n",
    "train_handle = sess.run(training_iterator.string_handle())\n",
    "heldout_handle = sess.run(heldout_iterator.string_handle())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newloss,newacc = sess.run([elbo_loss, accuracy],feed_dict={handle: train_handle})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradients = []#First pass over the data with SGD (keeping in memory all the gradients)\n",
    "for _ in range(0,total, N):\n",
    "    print(_)\n",
    "    grads = tf.gradients(newloss, tf.trainable_variables())\n",
    "    var_updates = []\n",
    "    var_list = tf.trainable_variables()\n",
    "    for grad, var in zip(grads, var_list):\n",
    "        var_updates.append(var.assign_sub(FLAGS.learning_rate * grad))\n",
    "    train_op = tf.group(*var_updates)\n",
    "    gradients.append(grads)\n",
    "    sess.run(train_op,feed_dict={handle: train_handle})\n",
    "        \n",
    "train_lossmisso[0],train_accbbb[0] = sess.run([elbo_loss, accuracy],feed_dict={handle: train_handle})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for step in range(FLAGS.max_steps):\n",
    "for step in range(1,10):\n",
    "    _ = sess.run([train_op, accuracy_update_op],feed_dict={handle: train_handle})\n",
    "    train_lossmisso[step],train_accbbb[step] = sess.run([elbo_loss, accuracy],feed_dict={handle: train_handle})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "misso = train_lossmisso[0:100]\n",
    "#save loss\n",
    "with open('newlosses/missoloss', 'wb') as fp:\n",
    "    pickle.dump(misso, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeptech",
   "language": "python",
   "name": "deeptech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
