% !TEX root = main.tex
\section{Convergence of the mini-batch EM algorithm}\label{sec:mbem}

\subsection{MBEM for a curved exponential family} \label{sssec:expo}
Let $\prm \in \rset^d$ be  an open parameter set. In the particular case where for all $i \in \inter$ and $z_i \in \Zset$, the function $\prm \to f_i(z_i,\prm)$ belongs to the curved exponential family, we assume that:
\begin{assumption_expo}
For all $i \in \inter$ and $\prm \in \prm$:
\begin{equation}
\log f_i(z_i,\prm) = H_i(z_i) -\psi_i(\prm) + \langle \tilde{S}_i(z_i), \phi_i(\prm)\rangle.
\end{equation}
where $\psi_i: \prm \mapsto \mathbb{R}$ and $\phi_i: \prm \mapsto \mathbb{R}^{d_i}$ are twice continuously differentiable functions of $\prm$, $H_i: \Zset \mapsto \mathbb{R}$ is a twice continuously differentiable function of $z_i$ and $\tilde{S}_i: \Zset \mapsto \mathsf{S}_i$ is a statistic taking its values in a convex subset $\mathsf{S}_i$ of $\mathbb{R}^{d_i}$ and such that $\int_{\Zset}{|\tilde{S}_i(z_i)|p_i(z_i,\prm) \mu_i(\dz_i)} < \infty$.
\end{assumption_expo}
\noindent Define for all $\prm \in \prm$ and $i \in \inter$ the function $\bar{s}_i: \prm \to \mathsf{S}_i$ as:
\begin{equation}
\bar{s}_i(\prm) \triangleq \int_{\Zset}{\tilde{S}_i(z_i)p_i(z_i,\prm) \mu_i(\dz_i)}.
\end{equation}
Define, for all $\prm \in \prm$ and $s = (s_i, i \in \inter) \in \mathsf{S}$ where $\mathsf{S} = \times_{n=1}^N \mathsf{S}_i$, the function $L(s; \prm)$ by:
\begin{equation}\label{curvedL}
L(s;\prm) \triangleq \sum_{i=1}^N{\psi_i(\prm)} - \sum_{i=1}^N{\langle s_i, \phi_i(\prm)\rangle}.
\end{equation}
\begin{assumption_expo}
There exist a function $\hat{\prm}: \mathsf{S} \mapsto \prm$ such that for all $s \in \mathsf{S}$, :
\begin{equation}\label{max_suff}
L(s;\hat{\prm}(s))\leq L(s;\prm).
\end{equation}
\end{assumption_expo}
In many models of practical interest for all $s \in \mathsf{S}$, $\prm \mapsto L(s,\prm)$ has a unique minimum. In the context of the curved exponential family, the MBEM algorithm can be formulated as follows:

\begin{algorithm}[H]
\textbf{Initialisation}: given an initial parameter estimate $\prm^0$, for all $i \in \inter$ compute $s^0_i = \bar{s}(\prm^0)$.\\
\textbf{Epoch e, Iteration k}: given the current estimate $\prm^{e,k-1}$:
\begin{enumerate}
\item Pick a set $I_{e,k}$ uniformly on $\{A \subset \inter, \operatorname{card}(A)=p\}$.
\item For $i \in \inter$, compute $s^k_{i}$ such as:
\begin{align}
 s^{e,k}_i &=
  \begin{cases}
   \bar{s}_i(\prm^{e,k-1})    & \text{if } i \in I_{e,k}.\\
   s^{e,k-1}_i                               & \text{otherwise}.
  \end{cases}
\end{align}

\item Set $\prm^{e,k} = \hat{\prm}(s^{e,k})$ where $s^{e,k} = (s^{e,k}_i, i \in \inter)$.

\end{enumerate}
\caption{mini-batch EM for a curved exponential family}
\label{alg:mbem-expo}
\end{algorithm}

\subsection{MBEM, oEM and oEM-vr}
To study those three algorithms we assume a slightly different exponential family as in \eqref{curvedL}:
\begin{equation}\label{curvedL}
L(s;\prm) \triangleq \psi(\prm) - \langle \sum_{i=1}^N{s_i}, \phi(\prm)\rangle.
\end{equation}
Denote $s = \sum_{i=1}^N{s_i}$ and $\bar{s}_i(\prm) \triangleq \E_{p(z_i, \prm)}[\tilde{S}(z_i)]$ and the functions $F: \mathsf{S} \to \mathsf{S}$ and $f_i: \mathsf{S} \to \mathsf{S}$ where for all $s \in  \mathsf{S}$, 
$$f_i(s) = \bar{s}_i (\hat{\prm}(s)) \quad \textrm{and} \quad F(s) = \sum_{i=1}^N \bar{s}_i (\hat{\prm}(s)) = \sum_{i=1}^N f_i (s)$$

At iteration $k$ of epoch $e$, the E-step of those three algorithms consists in picking a single data $i_k$ and:
\begin{itemize}
\item \textbf{oEM algorithm:}  $s^{e,k} = s^{e,k-1} + \rho_k ( \bar{s}_{i_k}(\prm^{e,k-1})  - s^{e,k-1} )$
\item \textbf{oEM-vr algorithm: }  $s^{e,k} = s^{e,k-1} + \rho ( \bar{s}_{i_k}(\prm^{e,k-1}) - f_{i_k}(s^{e,0})+ F(s^{e,0}) - s^{e,k-1} )$
\item \textbf{MBEM algorithm: }  $s^{e,k} = s^{e,k-1} +  ( \bar{s}_{i_k}(\prm^{e,k-1})  - s_{i_k}^{e,k-1} )$
\end{itemize}
with $ s^{e,0}  = s^{e-1,M}$.

\subsection{Local convergence of MBEM}
\begin{assumption}
For $i \in \inter$,  the function $\bar{s}_i: \prm \to \mathsf{S}_i$ is $L_s$-Lipschitz.
\end{assumption}

\begin{assumption}
The function $F: \mathsf{S} \to \mathsf{S}$ where for all $s \in  \mathsf{S}$, $F(s) = \sum_{i=1}^N \bar{s}_i (\hat{\prm}(s))$ is $\beta$-smooth.
\end{assumption}
\begin{assumption}\label{ass:local}
For any epoch $e$ and iteration $k$, $\norm{s^{e,k} - s^*} \leq \lambda/\beta$ where $1-\lambda$ is the maximum eigenvalue of $J_* = \frac{\partial F}{\partial s}(s^*)$ 
\end{assumption}

For all $i \in \inter$, define a random variable $z_i^{e,k}$ that takes value $1-\frac{1}{N}$ with probability $\frac{1}{N}$ and $-\frac{1}{N}$ otherwise and the associated vector $\barbelow{z}^{e,k} \in \rset^N$. Then for $i \in \inter$:
\begin{equation}
s_i^{e,k} = (1-\frac{1}{N}) s_i^{e,k-1} + \frac{1}{N}\bar{s}_{i_k}(\prm^{e,k-1}) + z_i^{e,k} \Big[\bar{s}_{i_k}(\prm^{e,k-1}) - s_i^{e,k-1}\Big]
\end{equation}
and
\begin{equation}
s^{e,k} =  \sum_{i=1}^N s_i^{e,k} = (1-\frac{1}{N})  \sum_{i=1}^N s_i^{e,k-1} + \frac{1}{N}  \sum_{i=1}^N \bar{s}_i(\prm^{e,k-1}) +  \sum_{i=1}^N z_i^{e,k} \Big[\bar{s}_i(\prm^{e,k-1}) - s_i^{e,k-1}\Big]
\end{equation}
Denote $\Delta_{e,k} = s^{e,k} - s^* \in \rset^{d}$ and  $\barbelow{z}^{e,k} = \begin{pmatrix} 
z_1^{e,k}  \\
\vdots  \\
z_N^{e,k}
\end{pmatrix} \in \rset^{N}$, then:
\begin{equation}
\begin{split}
\Delta_{e,k} & = s^{e,k} - s^* \\
& = \underbrace{(1-\frac{1}{N}) s^{e,k-1} - s^*  +  \frac{1}{N}  F(s^{e,k-1})}_{\Delta_{e,k}^{(1)}}  + \underbrace{(\barbelow{z}^{e,k})^\top \Big[ \barbelow{F}(s^{e,k-1}) - \barbelow{s}^{e,k-1} \Big]}_{\Delta_{e,k}^{(2)}}
\end{split}
\end{equation}
which leads to $\E[\norm{\Delta_{e,k}}^2] \leq \norm{\Delta_{e,k}^{(1)}}^2 + \E[\norm{\Delta_{e,k}^{(2)}}^2] $. Note that both $\barbelow{F}(s^{e,k-1}), \barbelow{s}^{e,k-1}$ are in $\rset^{N \times d}$.
Define the following quantity $J_* = \frac{\partial F}{\partial s}(s^*)$ and its maximum eigenvalue $1-\lambda$, we can bound the first term as follows:
\begin{equation}
\begin{split}
\Delta_{e,k}^{(1)} = (1-\frac{1}{N})\Delta_{e,k-1} + \frac{1}{N}J_* \Delta_{e,k-1} +  \frac{1}{N} \Big[ F(s^{e,k-1}) - s^* -J_* \Delta_{e,k-1}\Big]
\end{split}
\end{equation}
then:
\begin{equation}
\begin{split}
\norm{\Delta_{e,k}^{(1)}}^2 & = \norm{(1-\frac{1}{N})\Delta_{e,k-1} + \frac{1}{N} J_* \Delta_{e,k-1} +  \frac{1}{N} \Big[ F(s^{e,k-1}) - s^* -J_* \Delta_{e,k-1}\Big]}^2 \\
& \leq \Big[ \norm{(1-\frac{1}{N}) \Delta_{e,k-1} + \frac{1}{N}J_* \Delta_{e,k-1} } + \frac{1}{N} \norm{ F(s^{e,k-1}) - s^* -J_* \Delta_{e,k-1}}\Big]^2 \\
& \leq \Big[ (1-\frac{\lambda}{N})\norm{\Delta_{e,k-1}} + \frac{\beta}{2N} \norm{\Delta_{e,k-1}}^2 \Big]^2\\
& \leq \Big[ 1 - \frac{1}{N}(\lambda - \frac{\beta}{2} \norm{\Delta_{e,k-1}}) \Big]^2 \norm{\Delta_{e,k-1}}^2\\
& \leq (1 - \frac{\lambda}{2N})\norm{\Delta_{e,k-1}}^2
\end{split}
\end{equation}
where we use the triangular identity, the fact that $\norm{(1 - \frac{1}{N})I + \frac{1}{N} J_*} \leq 1 - \frac{1}{N} + \frac{1}{N}(1-\lambda) = 1 - \frac{\lambda}{N}$ and finally the smoothness of the function $F(s)$ yields $\norm{F(s) - s^* - J_* (s - s^*)} \leq \frac{\beta}{2}\norm{s - s^*}^2$ (for any smooth function $f$ we have $f(y) \leq f(x) + \nabla f(x) (y-x) + \beta/2 \norm{y-x}^2$.). The last line uses assumption M\ref{ass:local}.

Bounding the second term $\E[\norm{\Delta_{e,k}^{(2)}}^2] $ gives:
\begin{equation}
\begin{split}
\E[\norm{\Delta_{e,k}^{(2)}}^2]  & = \E[\operatorname{Tr}\left(\{\Delta_{e,k}^{(2)}\}^\top\Delta_{e,k}^{(2)}\right)]  \\
& =  \E \Big[\operatorname{Tr}\left(\Big\{ (\barbelow{z}^{e,k})^\top \Big[ \barbelow{F}(s^{e,k-1}) - \barbelow{s}^{e,k-1} \Big] \Big\}^\top (\barbelow{z}^{e,k})^\top \Big[ \barbelow{F}(s^{e,k-1}) - \barbelow{s}^{e,k-1} \Big] \right) \Big] \\
& = \operatorname{Tr}\left(\Big[ \barbelow{F}(s^{e,k-1}) - \barbelow{s}^{e,k-1} \Big]^\top \E[\barbelow{z}^{e,k} (\barbelow{z}^{e,k})^\top ]  \Big[ \barbelow{F}(s^{e,k-1}) - \barbelow{s}^{e,k-1} \Big]\right) \\
& = \operatorname{Tr}\left(\Big[ \barbelow{F}(s^{e,k-1}) - \barbelow{s}^{e,k-1} \Big]^\top [\frac{1}{N} I_{N} - \frac{1}{N^2} ee^\top]  \Big[ \barbelow{F}(s^{e,k-1}) - \barbelow{s}^{e,k-1} \Big]\right) \\
& \leq\operatorname{Tr}\left(\Big[ \barbelow{F}(s^{e,k-1}) - \barbelow{s}^{e,k-1} \Big]^\top\Big[ \barbelow{F}(s^{e,k-1}) - \barbelow{s}^{e,k-1} \Big]\right) \sqrt{\operatorname{Tr} \left[\left( \frac{1}{N} I_{N} - \frac{1}{N^2} ee^\top  \right)^2\right]} \\
& \leq  \frac{\sqrt{N-1}}{N} \norm{ \barbelow{F}(s^{e,k-1}) - \barbelow{s}^{e,k-1}}^2 \\
\end{split}
\end{equation}
where $e = \begin{pmatrix} 
1  \\
\vdots  \\
1
\end{pmatrix} \in \rset^{N}$. The third line is due to the linearity of the Trace operator and the line before the last uses the Cauchy Schwartz inequality.

We now need to upper bound the norm of the vector $\norm{ \barbelow{F}(s^{e,k-1}) - \barbelow{s}^{e,k-1}}^2$.
Using the Lipschitzness of $F$ and the fact that $s^*$ is a stationary point of $F$ we obtain:
\beq
\begin{split}
\norm{ \barbelow{F}(s^{e,k-1}) - \barbelow{s}^{e,k-1}}^2 &  = \norm{ \barbelow{F}(s^{e,k-1}) - F(s^*) + s^* - \barbelow{s}^{e,k-1}}^2 \\
& \leq \norm{\Delta_{e,k-1}}^2 + \norm{ \barbelow{F}(s^{e,k-1}) - F(s^*)}^2\\
& \leq \norm{\Delta_{e,k-1}}^2 +L_{F}^2\norm{\Delta_{e,k-1}}^2\\
& \leq (1+L_F^2) \norm{\Delta_{e,k-1}}^2
\end{split}
\eeq
\textcolor{red}{Rearranging terms yields
\beq
\begin{split}
\E[\norm{\Delta_{e,k}}^2]  & \leq \norm{\Delta_{e,k}^{(1)}}^2 + \E[\norm{\Delta_{e,k}^{(2)}}^2] \\
& \leq (1 - \frac{\lambda}{2N})\norm{\Delta_{e,k-1}}^2 +(1 - \frac{1}{N}) (1+L_F^2) \norm{\Delta_{e,k-1}}^2 \\
& \leq \left[ 1 - \frac{\lambda+2 +2  L_F^2}{2N}  +L_F^2 + 1\right] \norm{\Delta_{e,k-1}}^2
\end{split}
\eeq
Consider the last iteration $N$ of epoch $e$ ($\Delta_{e,0} = \Delta_{e-1,N}$), then:
\beq
\begin{split}
\E[\norm{\Delta_{e,N}}^2]  & \leq \left[ 1 - \left(\frac{\lambda+2 +2  L_F^2}{2N}  -(1+L_F^2)\right)\right]^N \norm{\Delta_{e,0}}^2 \\
& \leq \exp\left[- \left(\frac{\lambda}{2}+1 +  L_F^2  -N(1+L_F^2)\right)\right]\norm{\Delta_{e,0}}^2 
\end{split}
\eeq
Then:
\beq
\begin{split}
\E[\norm{s^{e,k} - s^*}^2]  & \leq \left\{\exp\left[- \left(\frac{\lambda}{2}+1 +  L_F^2  -N(1+L_F^2)\right)\right]\right\}^e \norm{s^{0,0} - s^*}^2\\
& \leq C^e\left\{\exp\left[N(1+L_F^2)\right]\right\}^e \norm{s^{0,0} - s^*}^2
\end{split}
\eeq
where $C = \exp[-(\frac{\lambda}{2}+1 +  L_F^2)] $.
}

\subsection{2 components GMM}
The sufficient statistics are noted: $s^{(1)}(y,z) \eqdef \indic_{z=1}, s^{(2)}_1(y,z) \eqdef y \indic_{z=1}, s^{(2)}_2(y,z) \eqdef y\indic_{z=2}$. The complete likelihood function reads:
\beq
\begin{split}
\ell(s,\prm) = & \left( \log(\pi) - \frac{\mu_1^2}{2\sigma^2} \right) s^{(1)} + \left( \log(1 - \pi) - \frac{\mu_2^2}{2\sigma^2} \right) (1 - s^{(1)}) + \frac{1}{\sigma^2} (\mu_1 s_1^{(2)} +  \mu_2 s_2^{(2)}) \\
&   - \lambda_{\pi} (\log(\pi) + \log(1-\pi)) +  \lambda_{\mu}  \frac{\mu_1^2}{2\sigma^2} +  \lambda_{\mu}  \frac {\mu_2^2}{2\sigma^2}
\end{split}
\eeq
The vector of parameters is $\prm = (\pi, \mu_1, \mu_2)$ and:
\begin{align*}
& \phi_1(\prm) =  \left\{ \log(\pi) - \frac{\mu_1^2}{2\sigma^2} \right\}  +   \left\{\log(1 - \pi) - \frac{\mu_2^2}{2\sigma^2} \right\} \\
& \phi_2(\prm) =  \frac{1}{\sigma^2} \mu_1  \\
& \phi_3(\prm) =   \frac{1}{\sigma^2} \mu_2  \\
& \psi(\prm) =  \frac{\mu_2^2}{2\sigma^2} + \log(1 - \pi) 
\end{align*}

Define the posterior probability of belonging to the first mixture component as follows:
\beq
w(y,\prm) \eqdef    \frac{  \pi {\rm exp}(-\frac{1}{2\sigma^2}(y - \mu_1)^2) }{  \sum_{j=1}^{2}{\pi_j \exp(-\frac{1}{2\sigma^2}(y - \mu_j)^2)} }
\eeq
At epoch $e$, the $(k+1)$th E-step of the incremental EM is given by:
\begin{align*}
    & (s^{e,k+1})^{(1)} = \sum_{i=1}^N (s_i^{e,k+1})^{(1)}  = (s^{e,k})^{(1)}+ \left(w(y_{i_{e,k+1}},\prm_{e,k})-(s_{i_{e,k}}^{e,k})^{(1)}\right)\\
    & (s^{e,k+1})^{(2),1} = \sum_{i=1}^N (s_i^{e,k+1})^{(2),1}  = (s^{e,k})^{(2),1}+ \left(y_{i_{e,k+1}}w(y_{i_{e,k+1}},\prm_{e,k})-(s_{i_{e,k}}^{e,k})^{(2),1}\right)\\
    & (s^{e,k+1})^{(2),2} = \sum_{i=1}^N (s_i^{e,k+1})^{(2),2}  = (s^{e,k})^{(2),2}+ \left(y_{i_{e,k+1}}w(y_{i_{e,k+1}},\prm_{e,k})-(s_{i_{e,k}}^{e,k})^{(2),2}\right)\\
\end{align*}
where $i_{e,k+1}$ is picked uniformly. And the M-step yields
\begin{align*}
    & \hat{\pi}_{n+1} =  \frac{(s^{e,k+1})^{(1)}+ \lambda_{\pi}}{1 + 2\lambda_{\pi}}   \\
    & \hat{\mu}_{1,n+1} =\frac{(s^{e,k+1})^{(2),1}}{(s^{e,k+1})^{(1)} + \lambda_{\mu}} \\
       & \hat{\mu}_{2,n+1} =\frac{(s^{e,k+1})^{(2),2}}{1-(s^{e,k+1})^{(1)}+ \lambda_{\mu}}
\end{align*}
\clearpage 
\begin{exmp}
We observe $N$ independent and identically distributed (i.i.d.) random variables $ (y_i, i \in \inter)$. Each one of these observations is distributed according to a mixture model. Denote by $(c^j, j \in \llbracket 1, J \rrbracket)$ the distribution of the component of the mixture and $(\pi_j, j \in \llbracket 1, J \rrbracket)$ the associated weights. Consider the complete data likelihood for each individual $f_i( z_i, \prm)$:
\begin{align} \label{eq1}
f_i( z_i, \prm) = \prod_{j=1}^{J}{(\pi_j c^j(y_i,\delta))^{\indic_{z_i=j}}}\eqs.
\end{align}
We restrict this study to a mixture of Gaussian distributions. In such case $\prm = ((\pi_j, \mu_j, \sigma_j), j \in \llbracket 1, J \rrbracket)$ and the individual complete log likelihood is expressed as:
\begin{align}
\log f_i( z_i, \prm) & = \sum_{j=1}^{J}{\indic_{z_i=j}\log(\pi_j)} + \sum_{j=1}^{J}{\indic_{z_i=j}\left[-\frac{(y_i - \mu_j)^2}{2\sigma^2_j} - \frac{1}{2}\log \sigma^2_j\right]}\eqs.
\end{align}
% Assumptions \cref{twicediff}\ref{complete} and \cref{bounded} are classically verified. With respect to $\pi_l$ the second derivative of the individual complete likelihood reads $\partial^2_{\pi_l} f_i(z_i,\prm)= 0$ and with respect to $\mu_l$ the second derivative of the individual complete likelihood reads $\partial^2_{\mu_l} f_i(z_i,\prm)= \frac{\indic_{z_i=l}}{\sigma^2_l}f_i(z_i,\prm)((y_i - \mu_l)^2 - 1)$.
The complete data sufficient statistics are given for all $i \in \inter$ and $j \in \llbracket1,J\rrbracket$, by $\tilde{S}_i^{1,j}(y_i,z_i) \triangleq \mathbb{1}_{z_i=j}$, $\tilde{S}_i^{2,j}(y_i,z_i) \triangleq \indic_{z_i=j}y_i$ and $\tilde{S}_i^{3,j}(y_i,z_i) \triangleq \indic_{z_i=j}y_i^2$.
At each iteration $k$, algorithm \ref{alg:mbem-expo} consists in picking a set $I_k$ and for $i \in I_k$, computing the following quantities:
\begin{align}
& (\bar{s}_i^k)^{1,j} = \int_{\Zset}{\indic_{z_i=j}p_i(z_i,\prm^{k-1}) \mu_i(\dz_i)} = p_{ij}(\prm^{k-1})\eqs,\\
& (\bar{s}_i^k)^{2,j} = \int_{\Zset}{\indic_{z_i=j}y_ip_i(z_i,\prm^{k-1}) \mu_i(\dz_i)} = p_{ij}(\prm^{k-1})y_i\eqs,\\
& (\bar{s}_i^k)^{3,j} = \int_{\Zset}{\indic_{z_i=j}y_i^2p_i(z_i,\prm^{k-1}) \mu_i(\dz_i)} = p_{ij}(\prm^{k-1})y_i^2\eqs,
\end{align}
where the quantity $p_{ij}(\prm^{k-1}) \triangleq \prob_{i,\prm^{k-1}}(z_i = j)$ is obtained using the Bayes rule:
\begin{equation}\label{gmm:condexpect}
p_{ij}(\prm^{k-1}) =\frac{\prob_i(z_i = j)p_i(y_i|z_i=j;\prm^{k-1})}{p_i(y_i; \prm^{k-1})}=\frac{\pi_j^{k-1} c^j(y_i;\mu^{k-1}_j,\sigma^{k-1}_j)}{\sum_{l=1}^{J}\pi_l^{k-1}c^l(y_i;\mu_l^{k-1}, \sigma_l^{k-1})}\eqs.
\end{equation}
For $i \notin I_k$, $j \in \llbracket 1, J \rrbracket$, and $d \in \llbracket 1,3 \rrbracket$ $(\bar{s}_i^k)^{d,j} = (\bar{s}_i^{k-1})^{d,j}$.
Finally the maximisation step yields:
\begin{align}
& \pi_j^k = \frac{\sum_{i=1}^{N}{(\bar{s}_i^k)^{1,j}}}{N}\eqs,\\
& \mu_j^k = \frac{\sum_{i=1}^{N}{(\bar{s}_i^k)^{2,j}}}{\sum_{i=1}^{N}{(\bar{s}_i^k)^{1,j}}} \eqs,\\
& \sigma_j^k = \frac{\sum_{i=1}^{N}{(\bar{s}_i^k)^{3,j}}}{\sum_{i=1}^{N}{(\bar{s}_i^k)^{1,j}}} - (\mu_j^k)^2\eqs.
\end{align}
\end{exmp}
